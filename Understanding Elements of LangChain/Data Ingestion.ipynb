{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'speech.txt'}, page_content=' What Is Speech?\\n\\nSpeech is the process of expressing thoughts, ideas, and emotions through the use of spoken words. It is a vital form of human communication and serves as one of the primary means by which individuals convey information, build relationships, and share experiences. Speech involves several components, including the physiological production of sounds, linguistic structures, and the social and cultural context in which communication occurs.\\n\\nComponents of Speech:\\n1. Physiological Aspect:\\n   - Speech begins in the brain, where thoughts are formulated and transmitted to motor areas controlling speech organs.\\n   - The vocal cords, tongue, lips, and other articulators work together to produce sounds.\\n\\n2. Linguistic Aspect:\\n   - Speech relies on the use of language, including vocabulary, grammar, and syntax.\\n   - It can vary across languages and dialects, each with unique rules and sounds.\\n\\n3. Acoustic Aspect:\\n   - Speech sounds are vibrations that travel through the air as sound waves.\\n   - Pitch, volume, and tone help convey emotion and emphasis.\\n\\n4. Perceptual Aspect:\\n   - Listeners interpret speech through auditory processing, recognizing phonemes, words, and sentence structures.\\n\\nFunctions of Speech:\\n1. Communication:\\n   - Speech allows individuals to share information, ask questions, and express needs and desires.\\n   \\n2. Social Interaction:\\n   - It plays a key role in establishing relationships and maintaining social bonds.\\n   - Speech can include greetings, apologies, compliments, and more.\\n\\n3. Emotional Expression:\\n   - Speech reflects emotions through tone, pitch, and volume, conveying feelings such as joy, sadness, or anger.\\n\\n4. Cognitive Processes:\\n   - Speech aids in thinking, learning, and problem-solving by allowing individuals to articulate and clarify thoughts.\\n\\nEvolution of Speech:\\n- Human speech likely evolved from primitive vocalizations and gestures in early hominins.\\n- The development of complex speech required physiological changes, such as the lowering of the larynx and increased brain capacity.\\n\\nSpeech Disorders:\\n- Speech production can be affected by conditions such as stuttering, aphasia, or dysarthria.\\n- These disorders may result from neurological, developmental, or physical impairments.\\n\\nTechnology and Speech:\\n1. Speech Recognition:\\n   - Advances in artificial intelligence have enabled the development of systems like voice assistants (e.g., Siri, Alexa) that recognize and interpret speech.\\n   \\n2. Speech Synthesis:\\n   - Technology can now generate human-like speech through text-to-speech programs.\\n\\nImportance of Speech in Society:\\n- Speech facilitates education, governance, art, and cultural expression.\\n- It plays a central role in shaping human history and collaboration.\\n')]\n"
     ]
    }
   ],
   "source": [
    "# Text Loader (Used for .txt files like \"speech.txt\")\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'Chapter.pdf', 'page': 0}, page_content='1.1 IntroductIon to computer SyStem\\nA computer is an electronic device that can be \\nprogrammed to accept data (input), process it and \\ngenerate result (output). A computer along with \\nadditional hardware and software together is called a \\ncomputer system.  \\nA computer system primarily comprises a central \\nprocessing unit (CPU), memory, input/output devices \\nand storage devices. All these components function \\ntogether as a single unit to deliver the desired output. \\nA computer system comes in various forms and sizes. \\nIt can vary from a high-end server to personal desktop, \\nlaptop, tablet computer, or a smartphone. \\nFigure 1.1 shows the block diagram of a computer \\nsystem. The directed lines represent the flow of data  \\nand signal between the components. \\n1.1.1 Central Processing Unit (CPU)\\nIt is the electronic circuitry of a computer that carries \\nout the actual processing and usually referred as the \\nbrain of the computer. It is commonly called processor \\nalso. Physically, a CPU can be placed on one or more \\nmicrochips called integrated circuits (IC). The ICs \\ncomprise semiconductor materials. \\n“A computer would deserve \\nto be called intelligent if it \\ncould deceive a human into \\nbelieving that it was human.” \\n–Alan Turing\\nChapter 1 \\nComputer System\\nIn this chapter\\n » Introduction to \\nComputer System\\n » Evolution of \\nComputer \\n » Computer Memory \\n » Data Transfer \\nbetween Memory \\nand CPU\\n » Data and \\nInformation\\n » Microprocessors\\n » Software\\n » Operating System\\nFigure 1.1: Components of a computer system\\nSecondary \\nStorage Devices\\nInput \\nDevice Control Unit \\n(CU)\\nArithmetic Logic \\nUnit (ALU)\\nPrimary \\nMemory\\nOutput \\nDevice\\nCentral Processing \\nUnit (CPU)\\nCh 1.indd   1 08-Apr-19   11:36:15 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 1}, page_content='Computer SCienCe – ClaSS xi\\n2\\nThe CPU is given instructions and data through \\nprograms. The CPU then fetches the program and data \\nfrom the memory and performs arithmetic and logic \\noperations as per the given instructions and stores the \\nresult back to memory.\\nWhile processing, the CPU stores the data as well \\nas instructions in its local memory called registers. \\nRegisters are part of the CPU chip and they are limited \\nin size and number. Different registers are used for \\nstoring data, instructions or intermediate results. \\nOther than the registers, the CPU has two main \\ncomponents — Arithmetic Logic Unit (ALU) and Control \\nUnit (CU). ALU performs all the arithmetic and logic \\noperations that need to be done as per the instruction in a \\nprogram. CU controls sequential instruction execution, \\ninterprets instructions and guides data flow through the \\ncomputer’s memory, ALU and input or output devices. \\nCPU is also popularly known as microprocessor. We will \\nstudy more about it in section 1.5. \\n1.1.2 Input Devices\\nThe devices through which control signals are sent \\nto a computer are termed as input devices. These \\ndevices convert the input data into a digital form that is \\nacceptable by the computer system. Some examples of \\ninput devices include keyboard, mouse, scanner, touch \\nscreen, etc., as shown in Figure 1.2. Specially designed \\nbraille keyboards are also available to help the visually \\nimpaired for entering data into a computer. Besides, we \\ncan now enter data through voice, for example, we can \\nuse Google voice search to search the web where we can \\ninput the search string through our voice.\\nData entered through input device is temporarily \\nstored in the main memory (also called RAM) of the \\ncomputer system. For permanent storage and future use, \\nthe data as well as instructions are stored permanently \\nin additional storage locations called secondary memory.\\n1.1.3 Output Devices\\nThe device that receives data from a computer system \\nfor display, physical production, etc., is called output \\ndevice. It converts digital information into human-\\nunderstandable form. For example, monitor, projector,   \\nheadphone, speaker, printer, etc. Some output devices \\nFigure 1.2: Input devices\\nScanner\\nTouch Screen\\nKeyboard\\nMouse\\nFigure 1.3: Output devices\\nSpeaker\\nPrinter\\n3D printer\\nDisplay monitor\\nCh 1.indd   2 08-Apr-19   11:36:15 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 2}, page_content='Computer SyStem\\n3\\nare shown in Figure 1.3. A braille display monitor is \\nuseful for a visually challenged person to understand \\nthe textual output generated by computers.\\nA printer is the most commonly used device to get \\noutput in physical (hardcopy) form. Three types of \\ncommonly used printers are inkjet, laserjet and dot \\nmatrix. Now-a-days, there is a new type of printer \\ncalled 3D-printer, which is used to build physical \\nreplica of a digital 3D design. These printers are being \\nused in manufacturing industries to create prototypes \\nof products. Their usage is also being explored in the \\nmedical field, particularly for developing body organs.\\n1.2 evolutIon of computer\\nFrom the simple calculator to a modern day powerful \\ndata processor, computing devices have evolved in a \\nrelatively short span of time. The evolution of computing \\ndevices in shown through a timeline in Figure 1.4\\nFigure 1.4: Timeline showing key inventions in computing technology\\nA punched card is a \\npiece of stiff paper that \\nstores digital data in \\nthe form of holes at \\npredefined positions.\\n \\nAbacus\\nAnalytic Engine\\nPascaline EDVAC/ENIAC\\nTabulating Machine Integrated Circuit\\n1970\\nTransistor\\n1947\\n1937500 BC\\n19451642\\n1890\\n1834\\nThe Turing machine concept was a \\ngeneral purpose programmable \\nmachine that was capable of solving \\nany problem by executing the \\nprogram stored on the punched cards.\\nComputing is attributed to \\nthe invention of ABACUS \\nalmost 3000 years ago. It \\nwas a mechanical device \\ncapable of doing simple \\narithmetic calculations only.\\nVacuum tubes were \\nreplaced by transistors \\ndeveloped at Bell Labs, \\nusing semiconductor \\nmaterials.\\nCharles Babbage invented \\nanalytical engine, a \\nmechanical computing device \\nfor inputting, processing, \\nstoring and displaying the \\noutput, which is considered \\nto form the basis of modern \\ncomputers.\\nHerman Hollerith designed \\na tabulating machine for \\nsummarising the data stored \\non the punched card. It is \\nstep towards programming.\\nBlaize Pascal invented a mechanical \\ncalculator known as Pascal calculator \\nor Pascaline to do addition and \\nsubtraction of two numbers directly \\nand multiplication and division through \\nrepeated addition and subtraction.\\nJohn Von Neumann introduced \\nthe concept of stored program \\ncomputer which was capable of \\nstoring data as well as program \\nin the memory. The EDVAC and \\nthen the ENIAC computers were \\ndeveloped based on this concept.\\nAn Integrated Circuit (IC) is \\na silicon chip which contains \\nentire electronic circuit on a \\nvery small area. The size of \\ncomputer drastically reduced \\nbecause of ICs.\\nTuring Machine\\nCh 1.indd   3 08-Apr-19   11:36:15 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 3}, page_content='Computer SCienCe – ClaSS xi\\n4\\nThe Von Neumann architecture is shown in Figure \\n1.5. It consists of a Central Processing Unit (CPU) \\nfor processing arithmetic and logical instructions, a \\nmemory to store data and programs, input and output \\ndevices and communication \\nchannels to send or receive the \\noutput data. Electronic Numerical \\nIntegrator and Computer (ENIAC) \\nis the first binary programmable \\ncomputer based on Von Neumann \\narchitecture. \\nDuring the 1970s, Large Scale Integration (LSI) of \\nelectronic circuits allowed integration of complete \\nCPU on a single chip, called microprocessor. Moore’s \\nLaw predicted exponential growth in the number \\nof transistors that could be assembled in a single \\nmicrochip. In 1980s, the processing power of computers \\nincreased exponentially by integrating around 3 million \\ncomponents on a small-sized chip termed as Very \\nLarge Scale Integration (VLSI). Further advancement in \\ntechnology has made it feasible to fabricate high density \\nof transistors and other components (approx 106 \\ncomponents) on a single IC called Super Large Scale \\nIntegration (SLSI) as shown in Figure 1.6. \\nIBM introduced its first personal computer (PC) for \\nthe home user in 1981 and Apple introduced Macintosh \\nFigure 1.5: Von Neumann architecture for \\n the computer \\n10,000,000,000\\n1,000,000,000\\n100,000,000\\n10,000,000\\n1,000,000\\n100,000\\n10,000\\n1,000\\n100\\n10\\n1\\n1940 1950 1960 1970 1980 1990 2000 2010 2020\\nIntel Microprocessors\\nDoubles every 2 years\\nInvention of the\\nTransistor\\n4004\\n8086\\n286\\n386\\n486486\\nPentium II Pentium IV \\nPentium III\\nCore 2 DUO Core i7\\nPentium \\nNumber of Transistors\\nper Integrated Circuit\\nFigure 1.6: Exponential increase in number of  transistors used in ICs over time\\nIn 1965, Intel co-\\nfounder Gordon Moore \\nintroduced Moore’s \\nLaw which predicted \\nthat the number of \\ntransistors on a chip \\nwould double every two \\nyears while the costs \\nwould be halved. \\nCh 1.indd   4 08-Apr-19   11:36:15 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 4}, page_content='Computer SyStem\\n5\\nmachines in 1984. The popularity of the PC surged \\nby the introduction of Graphical User Interface (GUI) \\nbased operating systems by Microsoft and others in \\nplace of computers with only command line interface, \\nlike UNIX or DOS. Around 1990s, the growth of World \\nWide Web (WWW) further accelerated mass usage of \\ncomputers and thereafter computers have become an \\nindispensable part of everyday life.\\nFurther, with the introduction of laptops, personal \\ncomputing was made portable to a great extent. This \\nwas followed by smartphones, tablets and other \\npersonal digital assistants. These devices have leveraged \\nthe technological advancements in processor \\nminiaturisation, faster memory, high speed data and \\nconnectivity mechanisms. \\nThe next wave of computing devices includes \\nthe wearable gadgets, such as smart watch, lenses, \\nheadbands, headphones, etc. Further, smart appliances \\nare becoming a part of the Internet of Things (IoT), by \\nleveraging the power of Artificial Intelligence (AI).\\n1.3 computer memory\\nA computer system needs memory to store the data and \\ninstructions for processing. Whenever we talk about the \\n‘memory’ of a computer system, we usually talk about the \\nmain or primary memory. The secondary memory (also \\ncalled storage device) is used to store data, instructions \\nand results permanently for future use.\\n1.3.1 Units of Memory\\nA computer system uses binary numbers to store and \\nprocess data. The binary digits 0 and 1, which are the \\nbasic units of memory, are called bits. Further, these \\nbits are grouped together to form words. A 4-bit word \\nis called a Nibble. Examples of nibble are 1001, 1010, \\n0010, etc. A two nibble word, i.e., 8-bit word is called a \\nbyte, for example, 01000110, 01111100, 10000001, etc.\\nLike any other standard unit, bytes are grouped \\ntogether to make bigger chunks or units of memory. \\nTable 1.1 shows different measurement units for digital \\ndata stored in storage devices.\\nCh 1.indd   5 08-Apr-19   11:36:16 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 5}, page_content='Computer SCienCe – ClaSS xi\\n6\\n1.3.2 Types of Memory\\nHuman beings memorise many things over a lifetime, and \\nrecall from memory to make a decision or some action.\\nHowever, we do not rely on our memory completely, and \\nwe make notes and store important data and information \\nusing other media, such as notebook, manual, journal, \\ndocument, etc. Similarly, computers have two types of \\nmemory   — primary and secondary. \\n(A) Primary Memory\\nPrimary memory is an essential component of \\na \\ncomputer system. Program and data are loaded into the \\nprimary memory before processing. The CPU interacts \\ndirectly with the primary memory to perform read or \\nwrite operation. It is of two types viz. (i) Random Access \\nMemory (RAM) and (ii) Read Only Memory (ROM).\\nRAM is volatile, i.e., as long as the power is supplied \\nto the computer, it retains the data in it. But as soon \\nas the power supply is turned off, all the contents of \\nRAM are wiped out. It is used to store data temporarily \\nwhile the computer is working. Whenever the computer \\nis started or a software application is launched, the \\nrequired program and data are loaded into RAM \\nfor processing. RAM is usually referred to as main \\nmemory and it is faster than the secondary memory or \\nstorage devices.\\nOn the other hand, ROM is non-volatile, which \\nmeans its contents are not lost even when the power is \\nturned off. It is used as a small but faster permanent \\nstorage for the contents which are rarely changed. For \\nexample, the startup program (boot loader) that loads \\nthe operating system into primary memory, is stored  \\nin ROM.\\n(B) Cache Memory\\nRAM is faster than secondary storage, but not as \\nfast \\nas a computer processor. So, because of RAM, a CPU \\nTable 1.1 Measurement units for digital data\\nUnit Description Unit Description\\nKB (Kilobyte) 1 KB = 1024 Bytes PB (Petabyte) 1 PB = 1024 TB\\nMB (Megabyte) 1 MB = 1024 KB EB (Exabyte) 1 EB = 1024 PB\\nGB (Gigabyte) 1 GB = 1024 MB ZB (Zettabyte) 1 ZB = 1024 EB\\nTB (Terabyte) 1 TB = 1024 GB YB (Yottabyte) 1 YB = 1024 ZB\\nSuppose there is a \\ncomputer with RAM \\nbut no secondary \\nstorage. Can we install \\na software on that \\ncomputer?\\nThink and Reflect\\nCh 1.indd   6 08-Apr-19   11:36:16 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 6}, page_content='Computer SyStem\\n7\\nmay have to slow down. To speed up the operations of \\nthe CPU, a very high speed memory is placed between \\nthe CPU and the primary memory known as cache. It \\nstores the copies of the data from frequently accessed \\nprimary memory locations, thus, reducing the average \\ntime required to access data from primary memory. \\nWhen the CPU needs some data, it first examines the \\ncache. In case the requirement is met, it is read from \\nthe cache, otherwise the primary memory is accessed.\\n(C) Secondary Memory\\nPrimary memory has limited storage \\ncapacity and \\nis either volatile (RAM) or read-only (ROM). Thus, a \\ncomputer system needs auxiliary or secondary memory \\nto permanently store the data or instructions for \\nfuture use. The secondary memory is non-volatile and \\nhas larger storage capacity than primary memory. It \\nis slower and cheaper than the main memory. But, it \\ncannot be accessed directly by the CPU. Contents of \\nsecondary storage need to be first brought into the main \\nmemory for the CPU to access. Examples of secondary \\nmemory devices include Hard Disk Drive (HDD), CD/\\nDVD, Memory Card, etc., as shown in Figure 1.7.\\nHowever, these days, there are secondary storage \\ndevices like SSD which support very fast data transfer \\nspeed as compared to earlier HDDs. Also, data transfer \\nbetween computers have become easier and simple due \\nto the availability of small-sized and portable flash or \\npen drives. \\n1.4 data tranSfer between memory and cpu\\nData need to be transferred between the CPU and \\nprimary memory as well as between the primary and \\nsecondary memory. \\nData are transferred between different components of \\na computer system using physical wires called bus. For \\nexample, bus is used for data transfer between a USB \\nport and hard disk or between a hard disk and main \\nmemory. Bus is of three types — (i) Data bus to transfer \\ndata between different components, (ii) Address bus to \\ntransfer addresses between CPU and main memory. \\nThe address of the memory location that the CPU wants \\nto \\nread or write from is specified in the address bus, \\nFigure 1.7: Storage devices \\nPen \\nDrive\\nCD/DVD\\nCh 1.indd   7 08-Apr-19   11:36:16 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 7}, page_content=\"Computer SCienCe – ClaSS xi\\n8\\nand (iii) Control bus to communicate  \\ncontrol signals between different \\ncomponents of a computer. All these \\nthree buses collectively make the \\nsystem bus, as shown in Figure 1.8.\\nAs the CPU interacts directly with \\nmain memory, any data entered from \\ninput device or the data to be accessed \\nfrom hard disk needs to be placed in the \\nmain memory for further processing. \\nThe data is then transferred between \\nCPU and main memory using bus.\\nThe CPU places on the address bus, the address of \\nthe main memory location from which it wants to read \\ndata or to write data. While executing the instructions, \\nthe CPU specifies the read or write control signal through \\nthe control bus. \\nAs the CPU may require to read data from main \\nmemory or write data to main memory, a data bus is \\nbidirectional. But the control bus and address bus are \\nunidirectional. To write data into memory, the CPU \\nplaces the data on the data bus, which is then written \\nto the specific address provided through the address \\nbus. In case of read operation, the CPU specifies the \\naddress, and the data is placed on the data bus by a \\ndedicated hardware, called memory controller. The \\nmemory controller manages the flow of data into and \\nout of the computer's main memory.\\n1.5 mIcroproceSSorS\\nIn earlier days, a computer's CPU used to occupy a large \\nroom or  multiple cabinets. However, with advancement \\nin technology, the physical size of CPU has reduced and \\nit is now possible to place a CPU on a single microchip \\nonly. A processor (CPU) which is implemented on a \\nsingle microchip is called microprocessor. Nowadays, \\nalmost all the CPUs are microprocessors. Hence, the \\nterms are used synonymously for practical purpose.\\nMicroprocessor  is a small-sized electronic component \\ninside a computer that carries out various tasks involved \\nin data processing as well as arithmetic and logical \\noperations. These days, a microprocessor is built over \\nan integrated circuit comprising millions of small \\ncomponents like resistors, transistors and diodes.\\n Figure 1.8: Data transfer between components through \\nsystem bus\\nCh 1.indd   8 08-Apr-19   11:36:16 AM\\n2024-25\\n\"), Document(metadata={'source': 'Chapter.pdf', 'page': 8}, page_content='Computer SyStem\\n9\\nMicroprocessors have evolved over time in terms \\nof their increased processing capability, decreasing \\nphysical size and reduced cost. Currently available \\nmicroprocessors are capable of processing millions of \\ninstructions per millisecond. Table 1.2 lists different \\ntypes of microprocessors along with their generation, \\ntime period, and underlying technology since their \\ninception in early 1970s.\\nTable 1.2 Generations of Microprocessor\\nGeneration Era Chip \\ntype\\nWord \\nsize\\nMaximum \\nmemory size\\nClock \\nspeed\\nCores Example*\\nFirst 1971-73 LSI 4/8  \\nbit\\n1 KB 108 KHz-\\n200 KHz\\nSingle Intel 8080\\nSecond 1974-78 LSI 8 bit 1 MB Upto 2 MHz Single Motorola 6800 \\nIntel 8085\\nThird 1979-80 VLSI 16 bit 16 MB 4 MHz - 6 \\nMHz\\nSingle Intel 8086\\nFourth 1981-95 VLSI 32 bit 4 GB Upto 133 \\nMHz\\nSingle Intel 80386\\nMotorola 68030\\nFifth 1995 till \\ndate\\nSLSI 64 bit 64 GB 533 MHz - \\n34 GHz\\nMulticore Pentium, \\nCeleron, Xeon\\n*few prominent examples are included. \\nActivity 1.1\\nThe maximum memory \\nsize of microprocessors \\nof different generations \\nare given at Table 1.2. \\nRepresent each of the \\nmemory size in terms \\nof power of 2.\\n1.5.1 Microprocessor Specifications\\nMicroprocessors are classified on the basis of different \\nfeatures which include chip type, word size, memory \\nsize, clock speed, etc. These features are briefly \\nexplained below:\\n(A) Word Size\\nWord size is the maximum number of bits that a \\nmicroprocessor can process at a time. Earlier, a word \\nwas of 8 \\nbits, as it was the maximum limit at that \\ntime. At present, the minimum word size is 16 bits and \\nmaximum word size is 64 bits.\\n(B) Memory Size\\nDepending upon the word size, the size \\nof RAM varies. \\nInitially, RAM was very small (4MB) due to 4/8 bits word \\nsize. As word size increased to 64 bits, it has become \\nfeasible to use RAM of size upto 16 Exabytes (EB).\\n(C) Clock Speed\\nComputers have an internal clock that generates \\npulses \\n(signals) at regular intervals of time. Clock speed simply \\nmeans the number of pulses generated per second by the \\nCh 1.indd   9 08-Apr-19   11:36:16 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 9}, page_content='Computer SCienCe – ClaSS xi\\n10\\nclock inside a computer. The clock speed indicates the \\nspeed at which the computer can execute instructions.\\nEarlier, it was measured in Hertz (Hz) and Kilohertz \\n(kHz). But with advancement in technology and chip \\ndensity, it is now measured in Gigahertz (GHz), i.e., \\nbillions of pulses per second.\\n(D) Cores\\nCore is a basic computation unit of the CPU. Earlier \\nprocessors had only one computation unit, thereby \\ncapable of performing only  one task at a time. With the \\nadvent of multicore processor, it has become possible \\nfor the computer to \\nexecute multiple tasks, thereby \\nincreasing the system’s performance. CPU with two, \\nfour, and eight cores is called dual-core, quad-core and \\nocta-core processor, respectively. \\n1.5.2 Microcontrollers\\nThe microcontroller is a small computing device which \\nhas a CPU, a fixed amount of RAM, ROM and other \\nperipherals all embedded on a single chip as compared \\nto microprocessor that has only a CPU on the chip. The \\nstructure of a microcontroller is shown in Figure 1.9. \\nKeyboard, mouse, washing machine, digital camera, \\npendrive, remote controller, microwave are few examples \\nof microcontrollers. As these are designed for specific \\ntasks only, hence their size as well as cost is reduced. \\nBecause of the very small size of the \\nmicrocontroller, it is embedded in another device \\nor system to perform a specific functionality. For \\nexample, the microcontroller in a fully automatic \\nwashing machine is used to control the washing cycle \\nwithout any human intervention. The cycle starts \\nwith the filling of water, after which the clothes are \\nsoaked and washed; thereafter the water is drained \\nand the clothes are spin dry. The simple use of \\nmicrocontroller has permitted repetitive execution \\nof tedious tasks automatically without any human \\nintervention, thereby saving precious time.\\n1.6 data and InformatIon\\nA computer is primarily for processing data. A computer \\nsystem considers everything as data, be it instructions, \\npictures, songs, videos, documents, etc. Data can also be \\nFigure 1.9: Structure of \\nmicrocontroller\\nClock CPU Memory\\nBus System\\nInput Output\\nI/O-ports\\nActivity 1.2\\nFind out the clock speed \\nof the microprocessor \\nof your computer and \\ncompare with that of \\nyour peers?\\nCh 1.indd   10 08-Apr-19   11:36:16 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 10}, page_content='Computer SyStem\\n11\\nraw and unorganised facts that are processed to get \\nmeaningful information.\\nSo understanding the concept of data along with \\nits different types is crucial to understand the overall \\nfunctioning of a computer. Sometimes people use the \\nterms data, information and knowledge interchangeably, \\nwhich is incorrect. \\n1.6.1 Data and Its Types\\nA computer system has many input devices, which \\nprovide it with raw data in the form of facts, concepts, \\ninstructions, etc., Internally everything is stored in \\nbinary form (0 and 1), but externally, data can be input \\nto a computer in the text form consisting of English \\nalphabets A–Z, a–z, numerals 0 – 9, and special symbols \\nlike @, #, etc. Data can be input in other languages too \\nor it can be \\nread from the files. The input data may \\nbe from different sources, hence it may be in different \\nformats. For example, an image is a collection of Red, \\nGreen, Blue (RGB) pixels, a video is made up of frames, \\nand a fee receipt is made of numeric and non-numeric \\ncharacters. Primarily, there are three types of data.\\n(A) Structured Data\\nData which follows a strict record structure and is easy \\nto comprehend is called structured data. Such data with \\npre-specified tabular format \\nmay be stored in a data file \\nto access in the future. Table 1.3 shows structured data \\nrelated to monthly attendance of students maintained \\nby the school.\\nTable 1.3 Structured data: Monthly attendance records of students\\nRoll No Name Month Attendance (in %)\\nR1 Mohan May 95\\nR2 Sohan May 75\\nR3 Sheen May 92\\nR4 Geet May 82\\nR5 Anita May 97\\nR1 Mohan July 98\\nR2 Sohan July 65\\nR3 Sheen July 85\\nR4 Geet July 94\\nR5 Anita July 85\\nCh 1.indd   11 08-Apr-19   11:36:16 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 11}, page_content='Computer SCienCe – ClaSS xi\\n12\\nIt is clear that such data is organised in row/column \\nformat and is easily understandable. Structured data \\nmay be sorted in ascending or descending order. In the \\nexample, attendance data is sorted in increasing order \\non the column ‘month’. Other examples of structured \\ndata include sales transactions, online railway ticket \\nbookings, ATM transactions, etc. \\n(B) Unstructured Data\\nData which are not organised \\nin a pre-defined record \\nformat is called unstructured data. Examples include \\naudio and video files, graphics, text documents, social \\nmedia posts, satellite images, etc. Figure 1.10 shows a \\nreport card with monthly attendance record details sent \\nto parents. Such data are unstructured as they consist \\nof textual contents as well as graphics, which do not \\nfollow a specific format.\\nFigure 1.10: Unstructured data: Monthly attendance record \\nSchool Logo\\nPrincipal’s SignatureGuardian’s Signature\\nAttended:\\nAttendance record for the month of July\\nCan you give some \\nmore examples of \\nunstructured data?\\nThink and Reflect\\nFigure 1.11: Semi-structured data: Month-wise total \\nattendance record maintained by the school\\nName: Mohan  Class: XI \\nName: Sohan  Class: XI \\nName: Sheen  Class: XI \\nName: Geet  Class: XI \\nName: Geet  \\nMonth: July  \\nMonth: July  \\nMonth: July  \\nMonth: May  \\nMonth: July  Class: XI \\nAttendance: 98 \\nAttendance: 65\\nAttendance: 85\\nAttendance: 82\\nAttendance: 94 \\n(C) Semi-structured Data\\nData which have no well-defined \\nstructure but \\nmaintains internal tags or markings to separate data \\nelements are called semi-structured data. Examples \\ninclude email document, HTML page, comma separated \\nvalues (csv file), etc. Figure 1.11 shows an example of \\nsemi-structured data containing student’s month-wise \\nattendance details. In this example, there is no specific \\nformat for each attendance \\nrecord. Here, each data value \\nis preceded by a tag (Name, \\nMonth, Class, Attendance) for \\nthe interpretation of the data \\nvalue while processing.\\nCh 1.indd   12 08-Apr-19   11:36:17 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 12}, page_content='Computer SyStem\\n13\\n1.6.2 Data Capturing, Storage and Retrieval\\nTo process data, we need to first input or capture \\nthe data. This is followed by its storage in a file or a \\ndatabase so that it can be used in the future. Whenever \\ndata is to be processed, it is first retrieved from the \\nfile or database so that we can perform further actions \\non it.\\n(A) Data Capturing\\nIt involves the process of gathering data from different \\nsources in the digital form. This capturing may vary \\nfrom simple instruments like keyboard, barcode readers \\nused at shopping outlets (Figure 1.12), comments or \\nposts over social media, remote sensors on an earth \\norbiting satellite, \\netc. Sometimes, heterogeneity among \\ndata sources makes data capturing a complex task.\\n(B) Data Storage \\nIt is the process of storing \\nthe captured data for \\nprocessing later. Now-a-days data is being produced at \\na very high rate, and therefore data storage has become \\na challenging task. However, the decrease in the cost \\nof digital storage devices has helped in simplifying \\nthis task. There are numerous digital storage devices \\navailable in the market like as shown in Figure 1.7.\\nData keeps on increasing with time. Hence, the \\nstorage devices also require to be upgraded periodically. \\nIn large organisations, computers with larger and \\nfaster storage called data servers are deployed to store \\nvast amount of data. Such dedicated computers help \\nin processing data efficiently. However, the cost (both \\nhardware and software) of setting up a data server as \\nwell as its maintenance is high, especially for small \\norganisations and startups.\\n(C) Data Retrieval \\nIt involves fetching data from the storage devices, for its \\nprocessing as per the user requirement. As databases \\ngrow, the challenges involved in search and retrieval of \\nthe \\ndata in acceptable time, also increase. Minimising \\ndata access time is crucial for faster data processing.\\n1.6.3 Data Deletion and Recovery\\nOne of the biggest threats associated with digital data is \\nits deletion. The storage devices can malfunction or crash \\ndown resulting in the deletion of data stored. Users can \\nFigure 1.12: Capturing \\ndata using barcode reader\\nActivity 1.3\\nVisit some of the places \\nlike bank, automobile \\nshowroom, shopping \\nmall, tehsil office, etc., \\nand find out 2 – 3 names \\nof tools or instruments \\nused to capture data in \\ndigital format.\\nCh 1.indd   13 08-Apr-19   11:36:17 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 13}, page_content='Computer SCienCe – ClaSS xi\\n14\\naccidentally erase data from storage devices, or a hacker \\nor malware can delete the digital data intentionally.\\nDeleting digitally stored data means changing the \\ndetails of data at bit level, which can be very time-\\nconsuming. Therefore, when any data is simply deleted, \\nits address entry is marked as free, and that much \\nspace is shown as empty to the user, without actually \\ndeleting the data. \\nIn case data gets deleted accidentally or corrupted, \\nthere arises a need to recover the data. Recovery of the \\ndata is possible only if the contents or memory space \\nmarked as deleted have not been overwritten by some \\nother data. Data recovery is a process of retrieving \\ndeleted, corrupted and lost data from secondary \\nstorage devices.\\nThere are usually two security concerns associated \\nwith data. One is its deletion by some unauthorised \\nperson or software. These concerns can be avoided \\nby limiting access to the computer system and using \\npasswords for user accounts and files, wherever \\npossible. There is also an option of encrypting files to \\nprotect them from unwanted modification.\\nThe other concern is related to unwanted recovery of \\ndata by unauthorised user or software. Many a times, \\nwe discard our old, broken or malfunctioning storage \\ndevices without taking care to delete data. We assume \\nthat the contents of deleted files are permanently \\nremoved. However, if these storage devices fall into the \\nhands of mischief-mongers, they can easily recover \\ndata from such devices; this poses a threat to data \\nconfidentiality. This concern can be mitigated by using \\nproper tools to delete or shred data before disposing off \\nany old or faulty storage device.\\n1.7 Software \\nTill now, we have studied about the physical \\ncomponents or the hardware of the computer system. \\nBut the hardware is of no use on its own. Hardware \\nneeds to be operated by a set of instructions. These \\nsets of instructions are referred to as software. It is that \\ncomponent of a computer system, which we cannot \\nActivity 1.4\\nExplore possible ways \\nof recovering deleted \\ndata or data from a \\ncorrupted device. \\nActivity 1.5\\nCreate a test file and \\nthen delete it using \\nShift+Delete from \\nthe keyboard. Now \\nrecover the file using \\nthe methods you have \\nexplored in Activity 1.4.\\nCh 1.indd   14 08-Apr-19   11:36:17 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 14}, page_content='Computer SyStem\\n15\\ntouch or view physically. It comprises the instructions \\nand data to be processed using the computer hardware. \\nThe computer software and hardware complete any \\ntask together.\\nThe software comprises a set of instructions which \\non execution deliver the desired outcome. In other \\nwords, each software is written for some computational \\npurpose. Some examples of software include operating \\nsystems like Ubuntu or Windows 7/10, word processing \\ntool like LibreOffice or Microsoft Word, video player like \\nVLC Player, photo editors like GIMP and LibreOffice \\ndraw. A document or image stored on the hard disk or \\npen drive is referred to as a soft-copy. Once printed, the \\ndocument or an image is called a hard-copy. \\n1.7.1 Need of Software \\nThe sole purpose of a software is to make the computer \\nhardware useful and operational. A software knows how \\nto make different hardware components of a computer \\nwork and communicate with each other as well as with \\nthe end-user. We cannot instruct the hardware of a \\ncomputer directly. Software acts as an interface between \\nhuman users and the hardware.\\nDepending on the mode of interaction with hardware \\nand functions to be performed, the software can be broadly \\nclassified into three categories viz. (i) System software, \\n(ii) Programming tools and (iii) Application software.\\n1.7.2 System Software\\nThe software that provides the basic functionality \\nto operate a computer by interacting directly with its \\nconstituent hardware is termed as system software. A \\nsystem software knows how to operate and use different \\nhardware components of a computer. It provides services \\ndirectly to the end user, or to some other software. \\nExamples of system software include operating systems, \\nsystem utilities, device drivers, etc.\\n(A) Operating System\\nAs the name implies, the operating system is a system \\nsoftware that operates the \\ncomputer. An operating \\nsystem is the most basic system software, without \\nwhich other software cannot work. The operating system \\nmanages other application programs and provides \\nHardware refers to the \\nphysical components \\nof the computer system \\nwhich can be seen and \\ntouched. For example, \\nRAM, keyboard, \\nprinter, monitor, CPU, \\netc. On the other hand, \\nsoftware is a set of \\ninstructions and data \\nthat makes hardware \\nfunctional to complete \\nthe desired task. \\nCh 1.indd   15 08-Apr-19   11:36:17 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 15}, page_content='Computer SCienCe – ClaSS xi\\n16\\naccess and security to the users of the system. Some \\nof the popular operating systems are Windows, Linux, \\nMacintosh, Ubuntu, Fedora, Android, iOS, etc. \\n(B) System Utilities\\nSoftware used for maintenance and \\nconfiguration of the \\ncomputer system is called system utility. Some system \\nutilities are shipped with the operating system for \\nexample disk defragmentation tool, formatting utility, \\nsystem restore utility, etc. Another set of utilities are \\nthose which are not shipped with the operating system \\nbut are required to improve the performance of the \\nsystem, for example, anti-virus software, disk cleaner \\ntool, disk compression software, etc.\\n(C) Device Drivers\\nAs the name signifies, the \\npurpose of a device driver is to \\nensure proper functioning of a particular device. When \\nit comes to the overall working of a computer system, \\nthe operating system does the work. But everyday new \\ndevices and components are being added to a computer \\nsystem. It is not possible for the operating system alone \\nto operate all of the existing and new devices, where each \\ndevice has diverse characteristics. The responsibility \\nfor overall control, operation and management of a \\nparticular device at the hardware level is delegated to \\nits device driver.\\nThe device driver acts as an interface between the \\ndevice and the operating system. It provides required \\nservices by hiding the details \\nof operations performed at the \\nhardware level of the device. Just \\nlike a language translator, a device \\ndriver acts as a mediator between \\nthe operating system and the \\nattached device. The categorisation \\nof software is shown in Figure 1.13.\\n1.7.3 Programming Tools\\nIn order to get some work done \\nby the computer, we need to give \\ninstructions which are applied on the \\ninput data to get the desired outcome.\\nComputer languages are developed \\nfor writing these instructions. Figure 1.13: Categorisation of software\\nActivity 1.6\\nAsk your teacher to \\nhelp you locate any two \\ndevice drivers installed \\non your computer.\\nCh 1.indd   16 08-Apr-19   11:36:17 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 16}, page_content='Computer SyStem\\n17\\nIt is important to understand here that computers and \\nhumans understand completely different languages. \\nWhile humans are able to write programs in high-level \\nlanguage, computers understand machine language. \\nThere is a continuous need for conversion from high \\nlevel to machine level language, for which translators \\nare needed. Also, to write the instruction, code editors \\n(e.g., IDLE in Python) are needed. We will briefly describe \\nhere the programming languages, language translators \\nand program development tools.\\n(A) Classification of Programming Languages\\nIt is very difficult for \\na human being to write \\ninstructions in the form of 1s and 0s. So different types \\nof computer programming languages are developed to \\nsimplify the coding. Two major categories of computer \\nprogramming languages are low-level languages and \\nhigh-level languages.\\nLow-level languages are  machine  dependent languages \\nand include machine language and assembly language. \\nMachine language uses 1s and 0s to write instructions \\nwhich are directly understood and executed by the \\ncomputer. But writing a code in machine language is \\ndifficult as one has to remember all operation codes \\nand machine addresses. Also finding errors in the code \\nwritten in machine language is difficult. \\nTo simplify the writing of code, assembly language \\nwas developed that allowed usage of English-like words \\nand symbols instead of 1s and 0s. But one major \\ndrawback of writing a code in this language is that the \\ncode is computer specific, i.e., the code written for one \\ntype of CPU cannot be used for another type of CPU.\\nHigh level languages are machine independent and \\nare simpler to write code into. Instructions are using \\nEnglish like sentences and each high level language \\nfollows a set of rules, similar to natural languages. \\nHowever, these languages are not directly understood \\nby the computer. Hence, translators are needed to \\ntranslate high-level language codes into machine \\nlanguage. Examples of high level language include C++, \\nJava, Python, etc.\\n(B) Language Translators\\nAs the computer can understand only machine \\nlanguage, a translator is needed to convert program \\nwritten in assembly or high level language to machine \\nnoteS\\nnoteS\\nCh 1.indd   17 08-Apr-19   11:36:17 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 17}, page_content='Computer SCienCe – ClaSS xi\\n18\\nlanguage. The program code written in assembly or \\nhigh-level language is called source code. The source \\ncode is converted by a translator into the machine \\nunderstandable form called object (machine) code as \\ndepicted in Figure 1.14. \\nAs we have different types of computer languages, \\ndifferent translators are needed to convert the source \\ncode to machine code. The three types of translators \\nused in computing systems are assembler, compiler \\nand interpreter.\\nThe translator used to convert the code written \\nin assembly language to machine language is called \\nassembler. Each assembler can understand a specific \\nmicroprocessor instruction set only and hence, the \\nmachine code is not portable.\\nWe also need translators to convert codes written \\nin high level language (source code) to machine \\nunderstandable form (machine code) for execution by \\nthe computer. Compiler converts the source code into \\nmachine code. If the code follows all syntactic rules of \\nthe language, then it is executed by the computer. Once \\ntranslated, the compiler is not needed. \\nAn interpreter translates one line at a time instead of \\nthe whole program at one go. Interpreter takes one line, \\nconverts it into executable code if the line is syntactically \\ncorrect, and then it repeats these steps for all lines in \\nthe source code. Hence, interpreter is always needed \\nwhenever a source code is to be executed.\\n(C) Program Development Tools\\nWhenever we decide to write a program, we need a text \\neditor. An editor is a software \\nthat allows us to create a \\ntext file where we type instructions and store the file as \\nthe source code. Then an appropriate translator is used \\nto get the object code for execution. In order to simplify \\nthe program development, there are software called \\nIntegrated Development Environment (IDE) consisting \\nof text editor, building tools and debugger. A program \\ncan be typed, compiled and debugged from the IDE \\ndirectly. Besides Python IDLE, Netbeans, Eclipse, Atom, \\nLazarus are few other examples of IDEs. Debugger, as \\nthe name implies, is the software to detect and correct \\nerrors in the source code.\\nCode in machine \\nlanguage\\n(Object Code)\\nFigure 1.14: Translator to \\nconvert source code into \\nobject code\\nLanguage \\ntranslater\\nCode in high \\nlevel language\\n(Source Code)\\nCh 1.indd   18 08-Apr-19   11:36:17 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 18}, page_content='Computer SyStem\\n19\\n1.7.4 Application Software\\nThe system software provides the core functionality of \\nthe computer system. However, different users need the \\ncomputer system for different purposes depending upon \\ntheir requirements. Hence, a new category of software \\nis needed to cater to different requirements of the end-\\nusers. This specific software that works on top of the \\nsystem software is termed as application software. There \\nare again two broad categories of application software — \\ngeneral purpose and customised application software.\\n(A)  General Purpose Software\\nThe application software developed for generic \\napplications, to cater to a bigger audience in general \\nare called general purpose software. Such ready-made \\napplication software can be used by end users as per \\ntheir requirements. For example, spreadsheet tool Calc \\nof LibreOffice can be used by any computer user to do \\ncalculation or to create account sheet. Adobe Photoshop, \\nGIMP, Mozilla web browser, iTunes, etc., fall in the \\ncategory of general purpose software.\\n(B) Customised Software\\nThese are custom or tailor-made application software, \\nthat are developed to meet the requirements of a specific \\norganisation or an individual. They are better suited to \\nthe needs of an individual or an organisation, considering \\nthat they are designed as per special requirements. Some \\nexamples of user-defined software include websites, \\nschool management software, accounting software, \\netc. It is similar to buying a piece of cloth and getting a \\ntailor-made garment with the fitting, colour, and fabric \\nof our choice.\\n1.7.5 Proprietary or Free and Open Source Software \\nThe developers of some application software provide \\ntheir source code as well as the software freely to the \\npublic, with an aim to develop and improve further with \\neach other’s help. Such software is known as Free and \\nOpen Source Software (FOSS). For example, the source \\ncode of operating system Ubuntu is freely accessible \\nfor anyone with the required knowledge to improve or \\nadd new functionality. More examples of FOSS include \\nPython, Libreoffice, Openoffice, Mozilla Firefox, etc. \\nSometimes, software are freely available for use but \\nA computer system \\ncan work without \\napplication software, \\nbut it cannot work \\nwithout system \\nsoftware. For example, \\nwe can use a computer \\neven if no word \\nprocessing software \\nis installed, but if no \\noperating system is \\ninstalled, we cannot \\nwork on the computer. \\nIn other words, the use \\nof computer is possible \\nin the absence of \\napplication software.\\nActivity 1.8\\nWith the help of your \\nteacher, install one \\nfree and open source  \\napplication software on \\nyour computer.\\nActivity 1.7\\nWith the help of your \\nteacher, install one \\napplication software in \\nyour computer.\\nCh 1.indd   19 21-May-19   4:32:01 PM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 19}, page_content='Computer SCienCe – ClaSS xi\\n20\\nsource code may not be available. Such software are \\ncalled freeware. Examples of freeware are Skype, Adobe \\nReader, etc. When the software to be used has to be \\npurchased from the vendor who has the copyright of the \\nsoftware, then it is a proprietary software. Examples of \\nproprietary software include Microsoft Windows, Tally, \\nQuickheal, etc. A software can be freeware or open \\nsource or proprietary software depending upon the \\nterms and conditions of the person or group who has \\ndeveloped and released that software.\\n1.8 operatIng SyStem\\nAn operating system (OS) can be considered to be a \\nresource manager which manages all the resources of a \\ncomputer, i.e., its hardware including CPU, RAM, Disk, \\nNetwork and other input-output devices. It also controls \\nvarious application software and device drivers, manages \\nsystem security and handles access by different users. \\nIt is the most important system software. Examples of \\npopular OS are Windows, Linux, Android, Macintosh \\nand so on.\\nThe primary objectives of an operating system are \\ntwo-fold. The first is to provide services for building and \\nrunning application programs. When an application \\nprogram needs to be run, it is the operating system \\nwhich loads that program into memory and allocates \\nit to the CPU for execution. When multiple application \\nprograms need to be run, the operating system decides \\nthe order of the execution. \\nThe second objective of an operating system is to \\nprovide an interface to the user through which the user \\ncan interact with the computer. A user interface is a \\nsoftware component which is a part of the operating \\nsystem and whose job is to take commands or inputs \\nfrom a user for the operating system to process.\\n1.8.1 OS User Interface\\nThere are different types of user interfaces each of which \\nprovides a different functionality. Some commonly used \\ninterfaces are shown in Figure 1.15.\\n(A) Command-based Interface\\nCommand-based interface requires a user to enter the \\ncommands to perform different tasks like creating, \\nFigure 1.15: Types of user  \\ninterface of OS\\nWhen a computer is \\nturned on, who brings \\nthe OS into RAM from \\nthe secondary storage?\\nThink and Reflect\\nCommand-based \\nInterface\\nGraphical User \\nInterface\\nTypes of User \\nInterface of \\nOperating System\\nTouch-based \\nInterface\\nVoice-based \\nInterface\\nGesture-based \\nInterface\\nCh 1.indd   20 08-Apr-19   11:36:17 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 20}, page_content='Computer SyStem\\n21\\nopening, editing or deleting a file, etc. The user has to \\nremember the names of all such programs or specific \\ncommands which the operating system supports.\\nThe primary input device used by the user for \\ncommand based interface is the keyboard. Command \\nbased interface is often less interactive and usually \\nallows a user to run a single program at a time.\\nExamples of operating systems with command-based \\ninterface include MS-DOS and Unix.\\n(B) Graphical User Interface\\nGraphical User Interface (GUI) lets users run programs \\nor give instructions to the computer in the form of \\nicons, menus and other visual options. Icons usually \\nrepresent files \\nand programs stored on the computer \\nand windows represent running programs that the user \\nhas launched through the operating system. \\nThe input devices used to interact with the GUI \\ncommonly include the mouse and the keyboard. \\nExamples of operating systems with GUI interfaces \\ninclude Microsoft Windows, Ubuntu, Fedora and \\nMacintosh, among others.\\n(C) Touch-based Interface\\nToday smartphones, tablets and PCs allow users to \\ninteract with the system simply using the touch input. \\nUsing the touchscreen, a user provides inputs to the \\noperating system, which are interpreted by the OS as \\ncommands like opening an app, closing an app, dialing \\na number, scrolling across apps, etc. \\nExamples of popular operating systems with touch-\\nbased interfaces are Android and iOS. Windows \\n8.1 and 10 also \\nsupport touch-based interfaces on \\ntouchscreen devices. \\n(D) Voice-based Interface\\nModern computers have been designed to address the \\nneeds of all types of users including people with special \\nneeds and people who want to \\ninteract with computers \\nor smartphones while doing some other task. For users \\nwho cannot use the input devices like the mouse, \\nkeyboard, and touchscreens, modern operating systems \\nprovide other means of human-computer interaction. \\nUsers today can use voice-based commands to make \\na computer work in the desired way. Some operating \\nnoteS\\nCh 1.indd   21 08-Apr-19   11:36:17 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 21}, page_content='Computer SCienCe – ClaSS xi\\n22\\nsystems which provide voice-based control to users \\ninclude iOS (Siri), Android (Google Now or “OK Google”), \\nMicrosoft Windows 10 (Cortana) and so on.\\n(E) Gesture-based Interface\\nSome smartphones based on Android and iOS as well \\nas laptops let users interact with the devices using \\ngestures like waving, tilting, \\neye motion and shaking. \\nThis technology is evolving faster and it has promising \\npotential for application in gaming, medicine and \\nother areas.\\n1.8.2 Functions of Operating System\\nNow let us explore the important services and tasks \\nthat an operating system provides for managing the \\ncomputer system.\\n(A) Process Management\\nWhile a computer system is operational, different tasks \\nare running simultaneously. A program is \\nintended to \\ncarry out various tasks. A task in execution is known \\nas process. We can activate a system monitor program \\nthat provides information about the processes being \\nexecuted on a computer. In some systems it can be \\nactivated using Ctrl+Alt+Delete. It is the responsibility \\nof operating system to manage these processes and get \\nmultiple tasks completed in minimum time. As CPU is \\nthe main resource of computer system, its allocation \\namong processes is the most important service of the \\noperating system. Hence process management concerns \\nthe management of multiple processes, allocation \\nof required resources, and exchange of information \\namong processes.\\n(B) Memory Management\\nPrimary or main memory of a computer system is \\nusually limited. The main task of \\nmemory management \\nis to give (allocate) and take (free) memory from running \\nprocesses. Since there are multiple processes running \\nat a time, there arises a need to dynamically (on-the-go) \\nallocate and free memory to the processes. Operating \\nsystem should do it without affecting other processes \\nthat are already residing in the memory and once the \\nprocess is finished, it is again the responsibility of the \\noperating system to take the memory space back for re-\\nOperating system is called \\nresource manager as it \\nmanages different resources \\nlike main memory, CPU, \\nI/O devices, so that each \\nresource is used optimally \\nand system performance does \\nnot deteriorate.\\nCh 1.indd   22 08-Apr-19   11:36:17 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 22}, page_content='Computer SyStem\\n23\\nutilisation. Hence, memory management concerns with \\nmanagement of main memory so that maximum memory \\nis occupied or utilised by large number of processes \\nwhile keeping track of each and every location within \\nthe memory as free or occupied. \\n(C) File Management \\nData and programs are stored \\nas files in the secondary \\nstorage of a computer system. File management involves \\nthe creation, updation, deletion and protection of these \\nfiles in the secondary memory. Protection is a crucial \\nfunction of an operating system, as multiple users can \\naccess and use a computer system. There must be a \\nmechanism in place that will stop users from accessing \\nfiles that belong to some other user and have not been \\nshared with them. File management system manages \\nsecondary memory, while memory management system \\nhandles the main memory of a computer system. \\n(D) Device Management \\nA computer system has many I/O devices and hardware \\nconnected to it. \\nOperating system manages these \\nheterogeneous devices that are interdependent. The \\noperating system interacts with the device driver and the \\nrelated software for a particular device. The operating \\nsystem must also provide the options for configuring \\na particular device, so that it may be used by an end \\nuser or some other device. Just like files, devices also \\nneed security measures and their access to different \\ndevices must be restricted by the operating system to \\nthe authorised users, software and other hardware only.\\nnoteS\\nSummary\\n• A computing device, also referred as computer, \\nprocesses the input data as per given instructions \\nto generate desired output. \\n• Computer system has four physical components \\nviz.\\n (i) CPU, (ii) Primary Memory, (iii) Input Device \\nand (iv) Output Devices. They are referred to as \\nhardware of computer.\\n• Computer system has two types of primary \\nmemories viz. (i) RAM, the volatile memory and \\n(ii) ROM, the non-volatile memory.\\nCh 1.indd   23 08-Apr-19   11:36:17 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 23}, page_content='Computer SCienCe – ClaSS xi\\n24\\nexercISe\\n1.  Name the software required to make a computer \\nfunctional. Write down its two primary services.\\n2.  How does the computer understand a program \\nwritten in high level language?\\n3.  Why is the execution time of the \\nmachine code less \\nthan that of source code?\\n4.  What is the need of RAM? How does it differ from \\nROM?\\n5.  What is the need for secondary memory?\\n6.  How do different components of the computer \\ncommunicate with each other?\\n7.  Draw the block diagram of \\na computer system. Briefly \\nwrite about the functionality of each component.\\n8.  What is the primary role of \\nsystem bus? Why is \\ndata bus is bidirectional while address bus is \\nunidirectional?\\n9.  Differentiate between proprietary software and \\nfreeware software. Name two software for each type.\\n• System bus is used to transfer data, addresses \\nand control signals between components of the \\ncomputer system.\\n• A microprocessor is a small-sized electronic \\ncomponent inside a computer that performs \\nbasic \\narithmetic and logical operations on data. \\n• Microcontroller is a small computing device which \\nhas a CPU, a \\nfixed amount of RAM, ROM and other \\nperipherals embedded on a single chip.\\n• Software is a set of instructions written to achieve \\nthe desired tasks and are mainly categorised \\nas system software, programming tools and \\napplication software. \\n• Hardware of a computer cannot function on its own. \\nIt needs software to be operational or functional.\\n• Operating system is an interface between the user \\nand the computer and supervises the working of \\ncomputer system, i.e., it \\nmonitors and controls the \\nhardware and software of the computer system.\\nnoteS\\nCh 1.indd   24 08-Apr-19   11:36:17 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 24}, page_content='Computer SyStem\\n25\\nnoteS10.  Write the main difference between microcontroller \\nand microprocessor. Why do smart home appliances \\nhave a microcontroller instead of microprocessor \\nembedded in them?\\n11.  Mention the different types of data that \\nyou deal \\nwith while browsing the Internet.\\n12.  Categorise the following data as structured, semi-\\nstructured and unstructured:\\n• Newspaper\\n• Cricket Match Score\\n• HTML Page\\n• Patient records in a hospital\\n13.  Name the input or output device used to do the \\nfollowing:\\na)  To output audio \\nb)  To enter textual data\\nc) To make hard copy of a text file\\nd)  To display the data or information \\ne) To enter audio-based command\\nf) To build 3D models \\ng) To assist a visually-impaired individual in \\nentering data\\n14.  Identify the category (system, application, \\nprogramming tool) of the following software: \\na)  Compiler\\nb)  Assembler\\nc) Ubuntu\\nd)  Text editor\\nexplore yourSelf\\n1.  Ask your teacher to help you locate any two device \\ndrivers installed on your computer.\\n2.  Write any two system software and two application \\nsoftware installed on your computer.\\n3.  Which microprocessor does your personal computer \\nhave? Which generation does it belong to?\\n4.  What is the clock speed of your microprocessor?\\n5.  Name any two devices in your school or home that \\nhave a microcontroller. \\nCh 1.indd   25 08-Apr-19   11:36:18 AM\\n2024-25\\n'), Document(metadata={'source': 'Chapter.pdf', 'page': 25}, page_content='Computer SCienCe – ClaSS xi\\n26\\n6.  Check the size of RAM and HDD of a computer in \\nyour school. Make a table and write their \\nsize in \\nBytes, Kilobytes, Megabytes and Gigabytes.\\n7.  List all secondary storage devices available at your \\nschool or home.\\n8.  Which operating system is installed on your \\ncomputer at home or school?\\nnoteS\\nCh 1.indd   26 08-Apr-19   11:36:18 AM\\n2024-25\\n')]\n"
     ]
    }
   ],
   "source": [
    "# Reading a PDF File\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Chapter.pdf\")\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://lilianweng.github.io/', 'title': \"Lil'Log\", 'description': 'Document my learning notes.', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n👋 Welcome to Lil’Log\\n\\n\\nHi, this is Lilian. I’m documenting my learning notes in this blog since 2017. Based on the number of grammar mistakes in my posts, you can tell how much ChatGPT is involved 😉.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReward Hacking in Reinforcement Learning\\n    \\n\\n\\nReward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a reward function.\\nWith the rise of language models generalizing to a broad spectrum of tasks and RLHF becomes a de facto method for alignment training, reward hacking in RL training of language models has become a critical practical challenge. Instances where the model learns to modify unit tests to pass coding tasks, or where responses contain biases that mimic a user’s preference, are pretty concerning and are likely one of the major blockers for real-world deployment of more autonomous use cases of AI models.\\n...\\n\\nDate: November 28, 2024  |  Estimated Reading Time: 37 min  |  Author: Lilian Weng\\n\\n\\n\\n\\nExtrinsic Hallucinations in LLMs\\n    \\n\\n\\nHallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\nIn-context hallucination: The model output should be consistent with the source content in context. Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so. This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.\\n...\\n\\nDate: July 7, 2024  |  Estimated Reading Time: 30 min  |  Author: Lilian Weng\\n\\n\\n\\n\\nDiffusion Models for Video Generation\\n    \\n\\n\\nDiffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task—using it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model. In comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs. 🥑 Required Pre-read: Please make sure you have read the previous blog on “What are Diffusion Models?” for image generation before continue here. ...\\n\\nDate: April 12, 2024  |  Estimated Reading Time: 20 min  |  Author: Lilian Weng\\n\\n\\n\\n\\nThinking about High-Quality Human Data\\n    \\n\\n\\n[Special thank you to Ian Kivlichan for many useful pointers (E.g. the 100+ year old Nature paper “Vox populi”) and nice feedback. 🙏 ]\\nHigh-quality data is the fuel for modern data deep learning model training. Most of the task-specific labeled data comes from human annotation, such as classification task or RLHF labeling (which can be constructed as classification format) for LLM alignment training. Lots of ML techniques in the post can help with data quality, but fundamentally human data collection involves attention to details and careful execution. The community knows the value of high quality data, but somehow we have this subtle impression that “Everyone wants to do the model work, not the data work” (Sambasivan et al. 2021).\\n...\\n\\nDate: February 5, 2024  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\\n\\n\\n\\n\\nAdversarial Attacks on LLMs\\n    \\n\\n\\nThe use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.\\n...\\n\\nDate: October 25, 2023  |  Estimated Reading Time: 33 min  |  Author: Lilian Weng\\n\\n\\n\\n\\nLLM Powered Autonomous Agents\\n    \\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\nPlanning Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks. Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results. Memory Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn. Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval. Tool use The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more. Fig. 1. Overview of a LLM-powered autonomous agent system. Component One: Planning A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\n...\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n\\n\\nPrompt Engineering\\n    \\n\\n\\nPrompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.\\n...\\n\\nDate: March 15, 2023  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\\n\\n\\n\\n\\nThe Transformer Family Version 2.0\\n    \\n\\n\\nMany new Transformer architecture improvements have been proposed since my last post on “The Transformer Family” about three years ago. Here I did a big refactoring and enrichment of that 2020 post — restructure the hierarchy of sections and improve many sections with more recent papers. Version 2.0 is a superset of the old version, about twice the length.\\nNotations Symbol Meaning $d$ The model size / hidden state dimension / positional encoding size. $h$ The number of heads in multi-head attention layer. $L$ The segment length of input sequence. $N$ The total number of attention layers in the model; not considering MoE. $\\\\mathbf{X} \\\\in \\\\mathbb{R}^{L \\\\times d}$ The input sequence where each element has been mapped into an embedding vector of shape $d$, same as the model size. $\\\\mathbf{W}^k \\\\in \\\\mathbb{R}^{d \\\\times d_k}$ The key weight matrix. $\\\\mathbf{W}^q \\\\in \\\\mathbb{R}^{d \\\\times d_k}$ The query weight matrix. $\\\\mathbf{W}^v \\\\in \\\\mathbb{R}^{d \\\\times d_v}$ The value weight matrix. Often we have $d_k = d_v = d$. $\\\\mathbf{W}^k_i, \\\\mathbf{W}^q_i \\\\in \\\\mathbb{R}^{d \\\\times d_k/h}; \\\\mathbf{W}^v_i \\\\in \\\\mathbb{R}^{d \\\\times d_v/h}$ The weight matrices per head. $\\\\mathbf{W}^o \\\\in \\\\mathbb{R}^{d_v \\\\times d}$ The output weight matrix. $\\\\mathbf{Q} = \\\\mathbf{X}\\\\mathbf{W}^q \\\\in \\\\mathbb{R}^{L \\\\times d_k}$ The query embedding inputs. $\\\\mathbf{K} = \\\\mathbf{X}\\\\mathbf{W}^k \\\\in \\\\mathbb{R}^{L \\\\times d_k}$ The key embedding inputs. $\\\\mathbf{V} = \\\\mathbf{X}\\\\mathbf{W}^v \\\\in \\\\mathbb{R}^{L \\\\times d_v}$ The value embedding inputs. $\\\\mathbf{q}_i, \\\\mathbf{k}_i \\\\in \\\\mathbb{R}^{d_k}, \\\\mathbf{v}_i \\\\in \\\\mathbb{R}^{d_v}$ Row vectors in query, key, value matrices, $\\\\mathbf{Q}$, $\\\\mathbf{K}$ and $\\\\mathbf{V}$. $S_i$ A collection of key positions for the $i$-th query $\\\\mathbf{q}_i$ to attend to. $\\\\mathbf{A} \\\\in \\\\mathbb{R}^{L \\\\times L}$ The self-attention matrix between a input sequence of lenght $L$ and itself. $\\\\mathbf{A} = \\\\text{softmax}(\\\\mathbf{Q}\\\\mathbf{K}^\\\\top / \\\\sqrt{d_k})$. $a_{ij} \\\\in \\\\mathbf{A}$ The scalar attention score between query $\\\\mathbf{q}_i$ and key $\\\\mathbf{k}_j$. $\\\\mathbf{P} \\\\in \\\\mathbb{R}^{L \\\\times d}$ position encoding matrix, where the $i$-th row $\\\\mathbf{p}_i$ is the positional encoding for input $\\\\mathbf{x}_i$. Transformer Basics The Transformer (which will be referred to as “vanilla Transformer” to distinguish it from other enhanced versions; Vaswani, et al., 2017) model has an encoder-decoder architecture, as commonly used in many NMT models. Later simplified Transformer was shown to achieve great performance in language modeling tasks, like in encoder-only BERT or decoder-only GPT.\\n...\\n\\nDate: January 27, 2023  |  Estimated Reading Time: 46 min  |  Author: Lilian Weng\\n\\n\\n\\n\\nLarge Transformer Model Inference Optimization\\n    \\n\\n\\n[Updated on 2023-01-24: add a small section on Distillation.]\\nLarge transformer models are mainstream nowadays, creating SoTA results for a variety of tasks. They are powerful but very expensive to train and use. The extremely high inference cost, in both time and memory, is a big bottleneck for adopting a powerful transformer for solving real-world tasks at scale.\\nWhy is it hard to run inference for large transformer models? Besides the increasing size of SoTA models, there are two main factors contributing to the inference challenge (Pope et al. 2022):\\n...\\n\\nDate: January 10, 2023  |  Estimated Reading Time: 9 min  |  Author: Lilian Weng\\n\\n\\n\\n\\nSome Math behind Neural Tangent Kernel\\n    \\n\\n\\nNeural networks are well known to be over-parameterized and can often easily fit data with near-zero training loss with decent generalization performance on test dataset. Although all these parameters are initialized at random, the optimization process can consistently lead to similarly good outcomes. And this is true even when the number of model parameters exceeds the number of training data points.\\nNeural tangent kernel (NTK) (Jacot et al. 2018) is a kernel to explain the evolution of neural networks during training via gradient descent. It leads to great insights into why neural networks with enough width can consistently converge to a global minimum when trained to minimize an empirical loss. In the post, we will do a deep dive into the motivation and definition of NTK, as well as the proof of a deterministic convergence at different initializations of neural networks with infinite width by characterizing NTK in such a setting.\\n...\\n\\nDate: September 8, 2022  |  Estimated Reading Time: 17 min  |  Author: Lilian Weng\\n\\n\\n\\n\\nGeneralized Visual Language Models\\n    \\n\\n\\nProcessing images to generate text, such as image captioning and visual question-answering, has been studied for years. Traditionally such systems rely on an object detection network as a vision encoder to capture visual features and then produce text via a text decoder. Given a large amount of existing literature, in this post, I would like to only focus on one approach for solving vision language tasks, which is to extend pre-trained generalized language models to be capable of consuming visual signals.\\n...\\n\\nDate: June 9, 2022  |  Estimated Reading Time: 25 min  |  Author: Lilian Weng\\n\\n\\n\\n\\nLearning with not Enough Data Part 3: Data Generation\\n    \\n\\n\\nHere comes the Part 3 on learning with not enough data (Previous: Part 1 and Part 2). Let’s consider two approaches for generating synthetic data for training.\\nAugmented data. Given a set of existing training samples, we can apply a variety of augmentation, distortion and transformation to derive new data points without losing the key attributes. We have covered a bunch of augmentation methods on text and images in a previous post on contrastive learning. For the sake of post completeness, I duplicate the section on data augmentation here with some edits. New data. Given few or even no data points, we can rely on powerful pretrained models to generate a number of new data points. This is especially true in recent years given the fast progress in large pretrained language models (LM). Few shot prompting is shown to be effective for LM to learn within context without extra training. Data Augmentation The goal of data augmentation is to modify the input format (e.g. text wording, visual appearance) while the semantic meaning stays unchanged.\\n...\\n\\nDate: April 15, 2022  |  Estimated Reading Time: 28 min  |  Author: Lilian Weng\\n\\n\\n\\n\\nLearning with not Enough Data Part 2: Active Learning\\n    \\n\\n\\n This is part 2 of what to do when facing a limited amount of labeled data for supervised learning tasks. This time we will get some amount of human labeling work involved, but within a budget limit, and therefore we need to be smart when selecting which samples to label.\\n...\\n\\nDate: February 20, 2022  |  Estimated Reading Time: 22 min  |  Author: Lilian Weng\\n\\n\\n\\n\\nLearning with not Enough Data Part 1: Semi-Supervised Learning\\n    \\n\\n\\n When facing a limited amount of labeled data for supervised learning tasks, four approaches are commonly discussed.\\n...\\n\\nDate: December 5, 2021  |  Estimated Reading Time: 26 min  |  Author: Lilian Weng\\n\\n\\n\\n\\nHow to Train Really Large Models on Many GPUs?\\n    \\n\\n\\n [Updated on 2022-03-13: add expert choice routing.] [Updated on 2022-06-10]: Greg and I wrote a shorted and upgraded version of this post, published on OpenAI Blog: “Techniques for Training Large Neural Networks”\\n...\\n\\nDate: September 24, 2021  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\\n\\n\\n\\n\\nWhat are Diffusion Models?\\n    \\n\\n\\n [Updated on 2021-09-19: Highly recommend this blog post on score-based generative modeling by Yang Song (author of several key papers in the references)]. [Updated on 2022-08-27: Added classifier-free guidance, GLIDE, unCLIP and Imagen. [Updated on 2022-08-31: Added latent diffusion model. [Updated on 2024-04-13: Added progressive distillation, consistency models, and the Model Architecture section.\\n...\\n\\nDate: July 11, 2021  |  Estimated Reading Time: 32 min  |  Author: Lilian Weng\\n\\n\\n\\n\\nContrastive Representation Learning\\n    \\n\\n\\n The goal of contrastive representation learning is to learn such an embedding space in which similar sample pairs stay close to each other while dissimilar ones are far apart. Contrastive learning can be applied to both supervised and unsupervised settings. When working with unsupervised data, contrastive learning is one of the most powerful approaches in self-supervised learning.\\n...\\n\\nDate: May 31, 2021  |  Estimated Reading Time: 39 min  |  Author: Lilian Weng\\n\\n\\n\\n\\nReducing Toxicity in Language Models\\n    \\n\\n\\n Large pretrained language models are trained over a sizable collection of online data. They unavoidably acquire certain toxic behavior and biases from the Internet. Pretrained language models are very powerful and have shown great success in many NLP tasks. However, to safely deploy them for practical real-world applications demands a strong safety control over the model generation process.\\n...\\n\\nDate: March 21, 2021  |  Estimated Reading Time: 23 min  |  Author: Lilian Weng\\n\\n\\n\\n\\nControllable Neural Text Generation\\n    \\n\\n\\n [Updated on 2021-02-01: Updated to version 2.0 with several work added and many typos fixed.] [Updated on 2021-05-26: Add P-tuning and Prompt Tuning in the “prompt design” section.] [Updated on 2021-09-19: Add “unlikelihood training”.]\\n...\\n\\nDate: January 2, 2021  |  Estimated Reading Time: 42 min  |  Author: Lilian Weng\\n\\n\\n\\n\\nHow to Build an Open-Domain Question Answering System?\\n    \\n\\n\\n [Updated on 2020-11-12: add an example on closed-book factual QA using OpenAI API (beta).\\nA model that can answer any question with regard to factual knowledge can lead to many useful and practical applications, such as working as a chatbot or an AI assistant🤖. In this post, we will review several common approaches for building such an open-domain question answering system.\\n...\\n\\nDate: October 29, 2020  |  Estimated Reading Time: 33 min  |  Author: Lilian Weng\\n\\n\\n\\n\\n »\\n\\n\\n\\n\\n© 2024 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4  # Ensure BeautifulSoup is imported\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(),\n",
    "        \n",
    "    )\n",
    ")\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://lilianweng.github.io/'}, page_content=\"\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\")]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4  # Ensure BeautifulSoup is imported\n",
    "\n",
    "# Define the class tuple\n",
    "class_tuple = ( \"header\", \"nav\")\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(class_=class_tuple)\n",
    "    )\n",
    ")\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'Published': '2016-08-31', 'Title': 'Collective Intelligence for Smarter API Recommendations in Python', 'Authors': \"Andrea Renika D'Souza, Di Yang, Cristina V. Lopes\", 'Summary': 'Software developers use Application Programming Interfaces (APIs) of\\nlibraries and frameworks extensively while writing programs. In this context,\\nthe recommendations provided in code completion pop-ups help developers choose\\nthe desired methods. The candidate lists recommended by these tools, however,\\ntend to be large, ordered alphabetically and sometimes even incomplete. A fair\\namount of work has been done recently to improve the relevance of these code\\ncompletion results, especially for statically typed languages like Java.\\nHowever, these proposed techniques rely on the static type of the object and\\nare therefore inapplicable for a dynamically typed language like Python. In\\nthis paper, we present PyReco, an intelligent code completion system for Python\\nwhich uses the mined API usages from open source repositories to order the\\nresults based on relevance rather than the conventional alphabetic order. To\\nrecommend suggestions that are relevant for a working context, a nearest\\nneighbor classifier is used to identify the best matching usage among all the\\nextracted usage patterns. To evaluate the effectiveness of our system, the code\\ncompletion queries are automatically extracted from projects and tested\\nquantitatively using a ten-fold cross validation technique. The evaluation\\nshows that our approach outperforms the alphabetically ordered API\\nrecommendation systems in recommending APIs for standard, as well as,\\nthird-party libraries.'}, page_content='Collective Intelligence for Smarter API\\nRecommendations in Python\\nAndrea Renika D’Souza, Di Yang, Cristina V. Lopes\\nDepartment of Informatics,\\nUniversity of California, Irvine\\n{ardsouza, diy, lopes}@uci.edu\\nAbstract—Software developers use Application Programming\\nInterfaces (APIs) of libraries and frameworks extensively while\\nwriting programs. In this context, the recommendations provided\\nin code completion pop-ups help developers choose the desired\\nmethods. The candidate lists recommended by these tools, how-\\never, tend to be large, ordered alphabetically and sometimes even\\nincomplete. A fair amount of work has been done recently to\\nimprove the relevance of these code completion results, especially\\nfor statically typed languages like Java. However, these proposed\\ntechniques rely on the static type of the object and are therefore\\ninapplicable for a dynamically typed language like Python.\\nIn this paper, we present PyReco, an intelligent code comple-\\ntion system for Python which uses the mined API usages from\\nopen source repositories to order the results based on relevance\\nrather than the conventional alphabetic order. To recommend\\nsuggestions that are relevant for a working context, a nearest\\nneighbor classiﬁer is used to identify the best matching usage\\namong all the extracted usage patterns.\\nTo evaluate the effectiveness of our system, the code completion\\nqueries are automatically extracted from projects and tested\\nquantitatively using a ten-fold cross validation technique. The\\nevaluation shows that our approach outperforms the alphabet-\\nically ordered API recommendation systems in recommending\\nAPIs for standard, as well as, third-party libraries.\\nI. INTRODUCTION\\nProgramming language ecosystems include several Appli-\\ncation Programming Interfaces (APIs), some of which are\\npart of standard libraries, while others come from third-party\\ndevelopers. These APIs help software developers extend the\\nfunctionality of their programs and improve software quality\\nwith very little additional code. However, learning how to use\\nthese APIs can sometimes take a lot of time. In a study with\\nMicrosoft developers, Robillard notes that most participants\\nattribute this difﬁculty of learning APIs with the scarce amount\\nof learning resources available [1].\\nCode completion is one of the most widely used features in\\nIntegrated Development Environments (IDEs) [2]. Stylos and\\nClarke observed that developers use this feature with the main\\nobjective of writing code faster, more correctly and to explore\\nAPIs [3].\\nThe ﬁrst generation code auto-complete tools in IDEs use\\nthe static type of an object to list out all possible attributes or\\nmethods that can be invoked or triggered when a user types\\na “.” character. However, M˘ar˘aoiu et al. [4] observed that\\na large fraction of the recommendations produced by these\\nearly recommenders were not being accepted by the users\\nand were being used more frequently for debugging purposes.\\nHence, recent research focuses towards building “intelligent”\\ncode completion tools which order the recommendations based\\non relevance rather than the conventional alphabetical order.\\nThese tools aim to improve the developer’s productivity and\\nsoftware quality by capturing the intent of a user for generating\\nthe completion lists.\\nWith its rapid development features, and simple and read-\\nable syntax, and powerful libraries, Python’s popularity has\\nbeen growing. Being a dynamic language, the traditional\\nintelligent techniques for code completion cannot be used for\\nrecommending APIs in Python. Variables in Python are not\\ngiven a type; instead, they take the type of whichever object\\nis assigned to them during run-time.\\nCurrent code completion tools in Python have been ineffec-\\ntive for API recommendations due to the following reasons:\\n• Long alphabetic lists of recommendations\\nMost code completion tools in Python display all the\\npossible methods and attributes which can be invoked\\nby an object, and these lists tend to be long. Some of the\\nrecommendations are rarely or never used by developers.\\nFor example, auto-complete invoked for a String object in\\nPython retrieves around 85 results by JEDI [6]. This large\\nalphabetically ordered list makes it hard to navigate to the\\ncorrect candidate, sometimes making it even slower than\\ntyping the full name of a method directly. Developers tend\\nto rely more on preﬁx ﬁltering than scrolling to reduce\\nthe number of choices [4].\\n• Scarce or ambiguous documentation of APIs\\nAPI documentation can be ambiguous, especially in ex-\\nplaining the type of the object returned by these APIs.\\nThis problem is complicated even further in Python since\\nthe language does not have static types. For instance,\\nthe documentation for urllib.open states that this method\\nwould return a ﬁle-like object. Developers may ﬁnd this\\ndescription confusing since it does not give a clear\\nindication of what methods or attributes can be invoked\\non the object returned.\\n• Incomplete static analysis of libraries\\nStatic analysis tools generate stubs for libraries which are\\nused to assist the development tools in recommending\\nand approximating the return types of API methods.\\nPyCharm [7] uses python skeletons [8] whereas Mypy [9]\\nis an optional static analysis tool that uses typeshed [10]\\nfor the stubs. These skeletons contain deﬁnitions for some\\narXiv:1608.08736v1  [cs.SE]  31 Aug 2016\\n(a) Monkey Patching[5]\\n(b) Union Types in PyCharm\\nFig. 1: Examples of Dynamic behavior in Python\\nof the most commonly used libraries especially the third-\\nparty ones. The list of libraries, however, is not complete\\nsince the generation of these stubs is highly dependent\\non the ability to perform static analysis on the source\\ncode of these libraries. According to Madsen [11], it\\nis complicated to statically analyze libraries due to the\\nfollowing reasons: (a) Software libraries may be partially\\nor fully implemented in another programming language;\\n(b) The source code for libraries may be large and not\\navailable for static analysis; and (c) Dynamic features\\nmay be used in the source code of these libraries. An\\nabsence of recommendations can, however, cause devel-\\nopers to suspect the presence of an error in the program\\nor to check the additional documentation available and\\nthus negatively impact their productivity [4].\\n(4) Failure to detect dynamic behavior\\nIn a dynamic language, it is possible that a variable\\ncan have a set of values at a particular point in the\\nprogram. For example, using Monkey Patching [12], APIs\\nof modules can be extended or modiﬁed using dynamic\\nbinding. In Figure 1a, the built-in function str is modiﬁed\\nto return a string object of type unicode instead of str.\\nSome code completion systems may fail to recognize\\nthese “union types” for an object. For example, in the\\ncode snippet shown in Figure 1b, the type of object\\nnamed a is guessed correctly to be both int and str in\\nPyCharm [7].\\nThese issues that exist in current code completion systems\\nfor Python sparks the need for new recommenders that have\\na greater understanding of developers’ goals and of Python’s\\ndynamic behavior in order to suggest APIs that are more suited\\nto the programming task.\\nIn this paper, we present PyReco1, an API recommender\\nthat uses the extracted API call sequences from open source\\nrepositories instead of conventional type inference techniques\\nfor the purpose of code completion. The intuition behind our\\napproach is that a large number of extracted API usage patterns\\npresent in these projects should be able to capture all the\\ndiverse scenarios in which APIs are currently being used by\\ndevelopers.\\nThe salient features of PyReco are as follows:\\n(1) A maximum of ten methods or attributes are recom-\\nmended for each completion query, and they are ranked\\nbased on relevance using our Nearest Neighbor classiﬁer,\\nBest Matching Object.\\n(2) Code recommendations become possible for all libraries\\nand APIs that were used in the mined open source\\nprojects. Thus, the completeness of this list is based on\\nthe popularity of libraries among developers rather than\\nthe ability to do static analysis on library code.\\n(3) Our approach for extracting the API usages leverages\\nthe semantics of the Python language and control ﬂow\\ninformation present in program to predict the dynamic\\nbehavior more accurately and capture the current working\\ncontext of the developer.\\nThe rest of the paper is organized as follows. In Section II,\\nwe discuss the related work done to improve code completion\\nresults. In Section III, we describe our approach to extract API\\nusage patterns and the Best Matching Object algorithm used\\nfor ordering the auto-complete proposals. In Section IV, the\\nevaluation procedure and metrics are described. In Section V,\\nwe present and discuss our experiment results and we conclude\\nin Section VI.\\n1https://github.com/Mondego/pyreco\\nII. RELATED WORK\\nIn the past few years, there has been fair amount of research\\ndone to improve the relevance of API recommendations by\\nusing context information, machine learning and statistical\\napproaches.\\nRobbes and Lanza [13] propose a code completion tool\\nthat uses temporal information like the program history to\\nprovide more relevant completions. On similar lines, Lee et\\nal. [14] have an additional temporal dimension for evolutionary\\ninformation on the code. In a collaborative work environment,\\nthey propose that such information could make development\\ntasks easier.\\nThe semantic or structural information in programs is most\\ncommonly used for context in recommenders. Heinemann et\\nal. [15] claim that the identiﬁer could be a good indicator for\\nthe methods that can be called for the development task. For\\ninstance, an object named angle could indicate the relevance\\nof suggesting sine and cosine operations. A context sensitive\\ncompletion approach by Asaduzzaman et al. [16] tokenizes\\nsemantic information like keywords, method, class or interface\\nnames from the preceding lines as part of the context to\\nimprove the relevance of the code completion results.\\nHou et al. [17] use a combination of grouping, sorting and\\nﬁltering techniques to improve code completion. In grouping,\\nthe APIs are grouped on the basis of their functionality. Sorting\\nis done based on type hierarchy and popularity, whereas\\nﬁltering is done to ﬁlter out APIs that aren’t public. However,\\nthese approaches require prior knowledge on the usage of each\\nAPI which is unfeasible due to the drastic increase in the\\nnumber of APIs in the past few years.\\nRaychev et al. [18] model the extracted method call se-\\nquences into statistical language models like N-Gram and\\nrecurrent neural networks to predict recommendations. This\\napproach has been proven to be fast and efﬁcient in determin-\\ning the likelihood of the next method call for Android APIs.\\nBruch et al. [19] propose a Best Matching Neighbor (BMN)\\nalgorithm which is used to identify the nearest neighbors\\namong the examples of API usages. These identiﬁed neighbors\\nare then used for recommendations.\\nBayesian networks is another machine learning model that\\nhas been used to predict the next most likely method for code\\ncompletion. Proksch et al.[20] use a Bayesian networks classi-\\nﬁer along with context information to determine the likelihood\\nof a method being invoked. These Pattern Based Bayesian\\nNetworks also incorporate clustering techniques to reduce the\\nmodel size and increase efﬁciency. These Bayesian networks\\nwere more effective than BMN for the SWT framework\\nin Java. McCarey et al. [21] also analyze the effectiveness\\nof using a Bayesian techniques for recommending library\\nmethods. However, the experiments show that a Vector Space\\nModel outperforms the Naive Bayes, Bayesian Network, Tree\\nAugmented Naive Bayes based classiﬁers.\\nHowever, all of the above proposed improvements have been\\nimplemented for a static typed language, speciﬁcally Java. The\\nmain objective of these approaches is to order the methods or\\nattributes for an object of a particular type.\\nf i l e\\n=\\n‘ samples / sample . t x t ’\\ntemp =\\n‘ samples / temp . t x t ’\\nf i\\n= open ( f i l e )\\nfo = open ( temp ,\\n‘w ’ )\\nf o r\\ns in\\nf i . readlines ( ) :\\ni f\\ns . s t r i p ( ) :\\nfo . write ( s )\\nelse :\\nfo . write ( ‘\\\\ n ’ )\\nbreak\\nfo . close ( )\\nf i . close ( )\\nFig. 2: Example code snippet\\nSch¨afer et al. [22] describe an approach based on static\\npointer analysis for smarter code completion results in\\nJavaScript, a dynamic language. However, this analysis is ﬂow-\\ninsensitive and thus, may not be able to detect some of the\\ndynamic behavior noticed in Python like union types. Also,\\nthe APIs models used in this static pointer analysis method are\\ngenerated after applying dynamic analysis on the framework’s\\ntest suite that is not available for most frameworks or libraries\\nfor Python.\\nFranks et al. [23] propose CACHECA that captures the\\nlocalized regularities in a program by using its recent token\\nusage frequency. This could, however could lead to some\\nfalse positives in the code suggestions for a dynamic language\\nwhich is not backed by types.\\nOur approach for ranking the recommendations is based\\non the BMN algorithm since it outperforms techniques which\\nincorporated association-rule mining [24] and statistical tech-\\nniques based on usage frequency.\\nIII. APPROACH\\nIn PyReco, we extract object usages from several GitHub\\nprojects and use a nearest neighbor classiﬁer on the extracted\\nusage patterns to order our recommendations based on rele-\\nvance.\\nThe ﬁrst phase of our implementation involves extracting\\nthe library object usages by applying static analysis on the\\nabstract syntax trees of the python source ﬁles. Our AST\\nParser uses the abstract syntax trees generated by the ast [25]\\nmodule in Python’s standard library. For the second phase, we\\npropose a Best Matching Object algorithm which is based on\\nBest Matching Neighbor (BMN) algorithm [19], to predict and\\norder the next most likely methods by using the mined usage\\ninformation.\\nTo illustrate our approach, we will use the code snippet\\ndescribed in Figure 2. The ﬁle handling example consists of\\ntwo ﬁle objects, ﬁand fo, which are created using the builtin\\nfunction, open. The example depicted shows a usage scenario\\nwherein fo, a temporary ﬁle is created to dump the data present\\nin ﬁ.\\nFig. 3: Program Graph generated for code snippet in Figure 2\\nA. Data Extraction\\nWe used the advanced search API [26] of GitHub to extract\\nopen source projects rated with the most number of stars in\\nPython. The number of stars in GitHub refers to the number of\\npeople watching the project. Since APIs and software libraries\\nare prone to change or become deprecated with time, the\\nchoice of such popular projects would be advantageous as they\\nwould be more likely to be updated with API changes than\\na less starred one. For our experiments, we extracted around\\n20,000 projects from GitHub [27].\\nB. Analysis of Python source ﬁles\\nIn this part, we recursively walk through the nodes of the\\nAST tree generated to analyze the python source ﬁles present\\nin the GitHub projects.\\nSome of the salient features of our analysis are as follows:\\n(1) The python source code ﬁles are parsed in a top-down\\nfashion. The top-down parsing emulates the way forward-\\ndirected completion is done in an IDE.\\n(2) The library and module information is extracted from the\\nImport nodes in the AST tree generated.\\n(3) An object assignment is added to the program’s graph if\\nit has been created using a library function.\\n(4) An API method or attribute is recorded in the graph if the\\nreceiver object’s assignment has been previously recorded\\nand the object is still alive in the current scope.\\n(5) The object’s death is marked when it is reassigned or\\nwhen its scope ends.\\nNo\\nNode\\nEntry Pts\\nExit Pts\\nReaching Defs\\n1\\nﬁbecomes open\\n2\\nﬁ:open\\n2\\nfo becomes open\\n1\\n3\\nfo:open, ﬁ:open\\n3\\nﬁcalls readlines\\n2\\n4,5\\nfo:open, ﬁ:open\\n4\\nfo calls write\\n3\\n6\\nfo:open, ﬁ:open\\n5\\nfo calls write\\n3\\n6\\nfo:open, ﬁ:open\\n6\\nfo calls close\\n3,4,5\\n7\\nfo:open, ﬁ:open\\n7\\nﬁcalls close\\n6\\n8\\nfo:open, ﬁ:open\\n8\\nfo dies\\n7\\n9\\nfo:open\\n9\\nﬁdies\\n8\\nTABLE I: Reaching Deﬁnitions and extracted context infor-\\nmation stored in Nodes\\nWe use graphs in our program analysis approach since it can\\nbe used to describe the assignments and calls in terms of the\\ncontrol ﬂow of a program. The program’s graph splits when\\na branching or looping construct is encountered and merges\\non exiting that block. This splitting and merging of ﬂows are\\ndepicted in the graph as shown in Figure 3. In the example\\nshown, the graph splits on encountering the if-else block and\\nthe for loop in the program. These ﬂows join after exiting the\\nscope of these blocks.\\nTo describe the control ﬂow of the program, each node\\ncontains information on its entry and exit nodes as shown\\nin Table I. These entry and exit nodes help in the traversing\\nacross all the execution ﬂows of the program and could be used\\nto approximate the set of values an object can have at different\\npoints of the program using Reaching Deﬁnition Analysis [28].\\nReaching Deﬁnitions analysis is done to determine all the\\ndeﬁnitions that reach a particular point in the program. At a\\nnode S in the graph, the reaching deﬁnitions S is the union of\\nthe reaching deﬁnitions from the entry nodes minus the ones\\nkilled at S (if an object dies at S) plus the deﬁnitions that are\\nadded in S (if an object is reassigned at S) [29]. For instance,\\nat node 6 (fo calls close) in Figure 3, the reaching deﬁnition\\nis a union of the deﬁnitions at 3, 4 and 5. No deﬁnition is\\nadded or subtracted at this node since the node at 6 marks a\\nclose call, not the death or assignment of an object.\\nThese reaching deﬁnitions can be used to detect “union\\ntypes”. For instance, if fo was assigned to os.path(ﬁle) in the\\nelse block, the reaching deﬁnitions at 6 for object fo would be\\na set of values containing os.path and open. Thus, we could\\nrecommend methods that are invoked on os.path as well as\\nopen.\\nCurrently, our approach tracks assignments made using\\nthe assignment operator (‘=’) or using ‘with’ construct. An\\nassignment node is added to the graph by evaluating the\\nright side of the assignment expression for a library call.\\nCertain assignments such as that of the iterator in for loops\\nis ignored since there is no substantial information on the\\ntype of the object. For instance, the object s in the for loop\\nshown in Figure 2 shows that it is a part of the iterator object\\nﬁ.readlines() but the statement does not clearly indicate the\\ntype of s.\\nWe are able to check “monkey patching” in most cases, by\\nevaluating the left side of the assignment expression to check if\\nFig. 4: Encoding the ﬁle objects as frequency vectors\\nFig. 5: Best Matching Objects from extracted Object Usages\\na library method has been modiﬁed or overridden. Such calls\\nare tracked and ignored while adding nodes to the program\\ngraph.\\nThe program graph created after parsing the syntax trees is\\nthen used to extract call sequences for training the recommen-\\ndation models. In the Figure 3, the extracted call sequence for\\nfo consists of write and close methods, whereas for ﬁ, the call\\nsequence consists of readlines and close methods.\\nC. Best Matching Object Algorithm\\nAfter extracting the object usages from all the GitHub\\n[27] projects, we train our models using a nearest neighbor\\nclassiﬁer, which is based on the Best Matching Neighbor\\nAlgorithm [19]. We name this algorithm as Best Matching\\nObject since it uses the contextual information speciﬁc to the\\nobject to recognize the nearest neighbors.\\nOur tailorings in this approach are as follows:\\n(1) Vectors are created for the training objects and query\\nbased on their invoking method-call frequency.\\n(2) Manhattan distance is used as the distance criteria to\\nselect the nearest neighbors.\\nThe basic approach involves creating vectors for the object\\nusages extracted from the projects, computing the Manhattan\\ndistance with the frequency vector created for the query, and\\nselecting the objects with the minimum distance from the\\nquery vector as the Nearest Neighbors. The recommendations\\nare then presented based on the decreasing order of frequencies\\nof methods invoked by the nearest neighbors.\\nThe calls to different methods tend to follow patterns or\\nchain sequences in a programming task. The object’s call\\nsequence is always present in these frequency vectors gen-\\nerated since they indicate about an object’s usage pattern. The\\ncall sequences for each object are recorded from the object’s\\nassignment nodes to the nodes marking its death through a\\ntraversal of the program graphs generated.\\nAs part of additional context, all the other methods that were\\ninvoked in the period between the creation and death of the\\nobject are recorded. These method calls are labeled as “other\\ncalls” in the frequency vectors shown in Figure 4.\\nA similar approach is followed while creating the frequency\\nvector for the query during the evaluation process. A backward\\ntraversal of the graph is done to retrieve all the information\\nrelated to the query object.\\nTo identify the nearest neighbors for the completion query,\\nwe compute the Manhattan distance between the query vector\\nand other similarly deﬁned objects found in our training\\nFig. 6: Code Recommendations using Nearest Neighbors\\nFig. 7: Evaluation using Cross Validation [20]\\ndataset. The distance measure is calculated by taking the sum\\nof the absolute values of the differences between the variables\\nof the two vectors as shown in the following formula:\\nd = Pn\\ni=1 |xi −yi|\\nManhattan Distance [30] was selected as the distance mea-\\nsure for measuring the similarity between the feature vectors\\nsince it performed much better than other measures like\\nEuclidean distance in our initial experiments.\\nTo calculate the Manhattan distance we considered only the\\ndifference in frequency between variables present in the query\\nvector. For instance, in the example shown in Figure 5, only\\nthe difference in the frequency of readlines is computed for\\nthe open objects. Thus, n in the distance equation refers to\\nthe number of methods in the query’s frequency vector with\\na usage frequency greater than 0.\\nUnlike the Best Matching Algorithm described by Bruch\\net al. [19], we kept the original frequency values instead of\\nreducing them to boolean factors since some methods tend to\\nbe called more frequently than others.\\nIn Figure 5, open obj1 and open obj2 are selected as the\\nnearest neighbors since they have the minimum Manhattan\\ndistance among the extracted open objects. The nearest neigh-\\nbors then vote based on their method frequencies. This vote\\nis done subtracting the method frequencies already present in\\nthe query vector. The methods that are recommended in the\\nexample query shown in the Figure 6 are readline, read and\\nclose.\\nIV. EVALUATION\\nIn this section, we will describe the process, metrics and\\nthe dataset that were used for measuring the effectiveness of\\nthe code recommenders.\\nA. Experiment Procedure\\nTo compare the prediction quality of PyReco to other\\nPython code recommenders, we propose an evaluation pro-\\ncedure consisting of automated case studies that are extracted\\nfrom the GitHub projects and manual case studies from the\\nlibrary’s ofﬁcial documentation and examples.\\nMost IDE tools depend on qualitative techniques and user\\nstudies to evaluate their effectiveness. However, it is very\\nhard to ﬁnd a representative set of users, and the approach\\ncould be time-consuming, costly and could result in subjec-\\ntive judgments [31], [32]. Automating this evaluation process\\ncould give a more objective idea of the performance of code\\ncompletion systems.\\nOur automated evaluation approach is based on the hold-\\nout validation process proposed by Bruch et. al [32]. The\\nautomated cases were tested using a 10-fold cross-validation\\nthat consists of the following main features:\\n(1) In each fold, 10% of the library objects are kept aside\\nas the validation set while the other 90% are used for\\ntraining.\\n(2) The validation set varies with each fold of the cross-\\nvalidation and queries for each fold are selected ran-\\ndomly. The intra-project completion queries were as-\\nsigned the same fold to avoid the positive bias and strong\\ncorrelation among the completion queries noticed in our\\ninitial experiments and prior research [31].\\n(3) The recommenders were queried for every method call\\nmade by these library objects by calling their code\\ncompletion API, thus making the process completely\\nautomated. The program snippet till the ”.” character at\\nwhich the method invoked is passed as input to the API.\\n(4) After each query, the evaluation metrics were calculated\\nwith the relevant set containing the method the user had\\noriginally used in the program. Thus, the relevant set in\\nour approach contains only one method.\\nJEDI [6], provides an API that can be used for retrieving\\nthe code completion results. This API is ideal for testing code\\ncompletion using the above-mentioned automated approach.\\nJEDI is a static analysis tool for Python which has been\\nintegrated into IDEs such as Atom. It has also been provided as\\na plugin for text editors such as Sublime. The code completion\\nresults by JEDI are alphabetically ordered.\\nManual case studies to test code recommenders usually\\nconsist of a few code completion scenarios that are identiﬁed\\nand validated by experts [31]. However, in the absence of such\\nexpert validated scenarios for Python, we tested each library\\nwith a code example found in its ofﬁcial documentation or\\nlearning resources 2.\\nIn these manual cases, we evaluated how PyReco fares\\nin comparison to PyCharm [7], IntelliJ’s plugin for Python,\\nwhich has a much more popular and powerful code completion\\nfeature than JEDI; however since their implementation is\\nclosed and it cannot be used for our automated experiments.\\nB. Experiment Dataset\\nIn the experiments proposed, the recommenders were eval-\\nuated for 20 Python libraries: 11 standard and 9 third-party.\\nThese libraries were chosen due to their popularity in Python’s\\ndevelopment community and the large numbers of library\\nobjects found in our dataset of software projects.\\nC. Evaluation Metrics\\nTo measure the quality of predictions made by the code\\nrecommenders, we used Mean Reciprocal Rank(MRR) and\\nRecall.\\nMean Reciprocal Rank is calculated by averaging the re-\\nciprocal of the rank at which the ﬁrst relevant document was\\n2https://github.com/Mondego/pyreco/tree/master/manual queries\\nfound across all the information needs [33]. For a set of code\\ncompletion queries Q, the Mean Reciprocal Rank is deﬁned\\nas:\\nMRR =\\n1\\n|Q|\\n|Q|\\nX\\ni=1\\n1\\nranki\\nSince this measure captures the rank of the relevant result,\\nit does not penalize the systems that retrieve long lists as\\nPrecision does. MRR is also more effective in measuring the\\nprediction quality in a ranked retrieval scenario as compared\\nto the other set-based measures. In our case, this measure\\nbecomes equivalent to Mean Average Precision (MAP)\\nsince our relevant set contains only one element, the method\\nthat the developer used in the program from which the query\\nwas generated.\\nWe use Recall to estimate the “completeness” of our results.\\nIt gives us an idea of the number of times the recommendation\\nlist did contain the method that the user was looking for.\\nRecall [28] is calculated using the following formula:\\nRecall = |{relevant documents} T {retrieved documents}|\\n|{relevant documents}|\\nAmong the measures described, we considered MRR as\\nthe main criteria to decide the quality of the results when the\\nmetrics have contrasting values.\\nTo illustrate the way these metrics are calculated, consider\\nthe code recommendations (readline, read, close) made in\\nFigure 6. If read was the method the user used for the query\\nshown in Figure 5, then the values of the metrics will be as\\nfollows:\\n(1) MRR = 0.5 since the rank at which read was found is\\n2.\\n(2) The Recall for this query is 1.0 since read was in the\\nset of retrieved recommendations.\\nV. RESULTS AND FINDINGS\\nIn this section, we will discuss the results and ﬁndings of our\\nexperiments conducted to assess the effectiveness of PyReco\\nas a code recommender.\\nA. Automated Queries\\nIn order to compare the performance of PyReco with JEDI,\\nwe sent the same set of completion queries for the standard\\nand third-party libraries using the 10 fold cross validation\\ntechnique described in Section IV. Each recommender is tested\\nwith an average of 30, 000 code completion queries sent across\\nten different splits for cross-validation. The evaluation metrics\\nwere averaged across the queries and are listed out in Table\\nII and Table III.\\nFor standard libraries, PyReco has an average MRR value of\\n0.5 which means that on an average, the relevant result appears\\nin the second position, whereas, JEDI has an average MRR\\nvalue of 0.11, which indicates that the relevant result appears\\nin the ninth position most of the times. The recall values show\\nthat the relevant result does not appear in more than half of\\nLibrary\\nPyReco-MRR\\nJEDI-MRR\\nPyReco-Recall\\nJEDI-Recall\\nos\\n0.592\\n0.037\\n0.943\\n0.356\\nre\\n0.727\\n0.196\\n0.967\\n0.853\\nctypes\\n0.369\\n0.146\\n0.565\\n0.161\\nlogging\\n0.425\\n0.080\\n0.730\\n0.615\\ndatetime\\n0.485\\n0.040\\n0.845\\n0.429\\ntime\\n0.516\\n0.0068\\n0.951\\n0.068\\njson\\n0.632\\n0.0137\\n0.950\\n0.068\\ncollections\\n0.418\\n0.161\\n0.776\\n0.665\\nstruct\\n0.646\\n0.237\\n0.927\\n0.843\\nsubprocess\\n0.560\\n0.260\\n0.925\\n0.741\\nargparse\\n0.306\\n0.424\\n0.422\\n0.518\\nTABLE II: Results for Python Standard Libraries\\nLibrary\\nPyReco-MRR\\nJEDI-MRR\\nPyReco-Recall\\nJEDI-Recall\\ndjango\\n0.467\\n0.001\\n0.687\\n0.003\\nnumpy\\n0.424\\n0.009\\n0.783\\n0.006\\nmock\\n0.252\\n0.000\\n0.472\\n0.000\\nsqlalchemy\\n0.551\\n0.092\\n0.871\\n0.419\\nPyQt4\\n0.559\\n0.000\\n0.896\\n0.000\\ntheano\\n0.674\\n0.000\\n0.930\\n0.000\\nwx\\n0.568\\n0.000\\n0.842\\n0.000\\ngoogle\\n0.638\\n0.001\\n0.910\\n0.002\\nﬂask\\n0.481\\n0.000\\n0.819\\n0.000\\nTABLE III: Results for Third-Party Libraries\\nthe code completion queries for JEDI. Thus, in addition to the\\nalphabetic order, the low recall value could also explain the\\nJEDI’s low MRR value since the MRR value is 0 in half the\\nqueries tested.\\nAmong the standard libraries, we notice that the recall and\\nMRR values for PyReco are low for mock and argparse.\\nSince our static analysis captures the data-ﬂow and call\\nsequences concerning only the library objects, we ﬁnd that\\nthese extracted sequences were not sufﬁcient to capture the\\ndeveloper’s working context for these libraries. For instance,\\npredicting the relevant method for an object returned by the\\nlibrary function ArgumentParser.parse args in the argparse,\\nwould require more information on the iterator value being\\npassed to this function. A mock library object is usually used\\nfor unit-testing, thus it could have variable methods that are\\nbased on the function being tested.\\nThese experiments also show the ineffectiveness of JEDI to\\nrecommend methods for third-party library objects. JEDI fails\\nto propose auto-complete suggestions for the code completion\\nqueries in most cases. This fact is corroborated by low MRR\\nand Recall values nearing 0. PyReco, on the other hand, has\\nan average Recall value of 0.84 which means that the relevant\\nresult is in the ten recommendations in 84% cases. The average\\nMRR value for PyReco, like the standard libraries, is around\\n0.5.\\nAmong the results for the third-party libraries, we notice\\nthat the library, ctypes has a comparatively lower recall and\\nMRR value. In ctypes, a CDLL object is created using a string\\ncontaining the name of C library. Since our static analysis does\\nnot capture this string, the relevant result could be absent from\\nthe recommendations made by PyReco.\\nThe low values for JEDI could be due to its inability to\\nrecognize “union types” and recommend for library objects\\nthat are not documented. This can be observed in the high\\nrecall values for re, struct and subprocess as compared to the\\nthird-party libraries.\\nThe results for standard and third-party libraries show that\\nPyReco does outperform JEDI in terms of prediction quality\\nand completeness of recommendations by a huge margin.\\nHowever, the results of this experiment can not be used to ex-\\nplain the impact of a relevance based ordering since JEDI fails\\nto recommend methods for most of the completion queries.\\nThe results, however, does indicate that PyReco captures the\\ndeveloper’s context and approximates the query object’s type\\nmuch more effectively than JEDI does.\\nB. Manual Queries\\nSince JEDI was not very effective in predicting methods for\\nthird-party libraries, we conducted experiments with PyCharm\\nto assess the effectiveness of a relevance-based ranking as\\ncompared to the conventional alphabetic order.\\nFor each of the above-mentioned 20 libraries, we tested\\nrecommender with an example found in the library’s online\\ndocumentation or learning resources. The rank at which the\\nrelevant method was found in completion pop-up was recorded\\nas shown in Table IV.\\nIn 16 of the 20 cases tested, the ordering of the recommen-\\ndations by PyReco was found to be better than PyCharm. Since\\nrecommendations made by PyReco are on the ﬁrst page of the\\npop-up, it could positively impact a developer’s productivity.\\nWe noticed that in 7 examples, the relevant result was not\\nfound on the ﬁrst page of PyCharm’s auto-complete pop-up.\\nOn the other hand, PyReco places the relevant result in the\\nﬁrst or second position in these seven examples.\\n3no recommendations were made for the completion query\\nLibrary\\nPyReco\\nPyCharm\\ndjango\\n1\\n25\\nos\\n1\\n7\\nnumpy\\n1\\n51\\nre\\n1\\n4\\nmock\\n0\\n03\\nctypes\\n2\\n03\\nlogging\\n8\\n3\\nsqlalchemy\\n5\\n3\\ndatetime\\n1\\n22\\ntime\\n7\\n0\\nPyQt4\\n8\\n0\\ntheano\\n3\\n7\\nwx\\n1\\n57\\njson\\n2\\n9\\ncollections\\n1\\n7\\nstruct\\n2\\n6\\nsubprocess\\n1\\n28\\ngoogle\\n2\\n03\\nﬂask\\n3\\n95\\nargparse\\n2\\n55\\nTABLE IV: Ranks for the relevant method in PyReco and\\nPyCharm\\nFor libraries like mock, ctypes and google, PyCharm fails\\nto provide any recommendations. This inability could be due\\nto the absence of stubs or skeletons for these libraries.\\nThe results from these manual cases studies do indicate that\\na relevance based ordering like the one in PyReco could be\\nmore useful for recommending APIs.\\nC. Threats to Validity\\nWe identiﬁed the following threats in our described\\napproach:\\n(1) Generalization based on tested libraries\\nOur results are summarized based on our experiments\\nusing 20 library methods. However, these ﬁndings could\\nbe challenged when other libraries are used. Our choice\\nof the libraries was based on the high frequency of\\nusage patterns detected in our dataset of projects and\\nits popularity in the Python development community.\\n(2) Generalization based on manual evaluation\\nOur results for the manual queries are summarized\\nbased on a single query. A single query may not be\\nrepresentative of the performance of a library. These\\nqueries were only used to evaluate how PyReco fares\\nagainst PyCharm since an automated evaluation was not\\nfeasible in the latter.\\n(3) Presence of bugs in repositories\\nThere is a possibility that some of the source code ﬁles\\nused for training may contain bugs and thus may lead to\\nsome false positives in the recommendations. However,\\nsince the projects extracted are the top starred ones, the\\npresence of these bugs is expected to be negligible.\\nVI. CONCLUSION AND FUTURE WORK\\nIn this paper, we described PyReco, a code recommender,\\nto help software developers explore APIs of libraries and\\nframeworks more effectively and efﬁciently in Python.\\nThe proposed recommender addresses the challenge of\\nrecommending APIs in a dynamic language by reusing the\\nintelligence found in open source repositories on API usages\\nand suggests method calls that are ordered by relevance, unlike\\nthe other code completion tools currently available for Python.\\nOur experimental results show that the predictions made by\\nPyReco are much more precise and complete than those made\\nby JEDI [6]. They also show promise when compared with\\nPyCharm [7] for the standard and third party libraries tested.\\nIn our experiments, we noticed capturing the call sequences\\nwas not sufﬁcient to capture the developer’s working context\\nfor libraries like ctypes, mock and argparse. To improve the\\nauto-complete predictions for these libraries, we could extract\\nmore information on the values of the primitive objects present\\nin the program.\\nWe plan to integrate PyReco as a plugin in the Integrated\\nDevelopment Tools (IDEs) for Python. However, in order to do\\nthat, we need to conduct user studies with software developers\\nto assess the usefulness of such a plugin and for directions in\\nits design.\\nACKNOWLEDGMENTS\\nThis work was partly supported by a grant from the DARPA\\nMUSE program.\\nREFERENCES\\n[1] M. P. Robillard, “What makes apis hard to learn? answers from devel-\\nopers,” Software, IEEE, vol. 26, no. 6, pp. 27–34, 2009.\\n[2] G. C. Murphy, M. Kersten, and L. Findlater, “How are java software\\ndevelopers using the elipse ide?” Software, IEEE, vol. 23, no. 4, pp.\\n76–83, 2006.\\n[3] J. Stylos and S. Clarke, “Usability implications of requiring parameters\\nin objects’ constructors,” in Proceedings of the 29th international\\nconference on Software Engineering.\\nIEEE Computer Society, 2007,\\npp. 529–539.\\n[4] M. M˘ar˘aoiu, L. Church, and A. Blackwell, “An empirical investigation\\nof code completion usage by professional software developers.”\\n[5] “monkeypatching - monkey patching in python: When we need\\nit?\\n-\\nstack\\noverﬂow,”\\nhttp://stackoverﬂow.com/questions/11977270/\\nmonkey-patching-in-python-when-we-need-it,\\n(Accessed\\non\\n05/17/2016).\\n[6] “Jedi - an awesome autocompletion/static analysis library for python\\njedi 0.9.0 documentation,” http://jedi.jedidjah.ch/en/latest/, (Accessed on\\n05/17/2016).\\n[7] “Pycharm,”\\nhttps://www.jetbrains.com/pycharm/,\\n(Accessed\\non\\n05/17/2016).\\n[8] “Jetbrains/python-skeletons: Collection of python ﬁles that contain api\\ndeﬁnitions of third-party libraries extended for python static analy-\\nsis tools,” https://github.com/JetBrains/python-skeletons, (Accessed on\\n05/17/2016).\\n[9] “python/mypy: Optional static typing for python,” https://github.com/\\npython/mypy, (Accessed on 05/17/2016).\\n[10] “python/typeshed: Collection of library stubs for python, with static\\ntypes,” https://github.com/python/typeshed, (Accessed on 05/17/2016).\\n[11] M. Madsen, “Static analysis of dynamic languages,” Ph.D. dissertation,\\nAarhus UniversitetAarhus University, Science and TechnologyScience\\nand Technology, Institut for DatalogiDepartment of Computer Science,\\n2015.\\n[12] “Monkey patch - wikipedia, the free encyclopedia,” https://en.wikipedia.\\norg/wiki/Monkey\\\\ patch, (Accessed on 05/17/2016).\\n[13] R. Robbes and M. Lanza, “How program history can improve code\\ncompletion,” in Automated Software Engineering, 2008. ASE 2008. 23rd\\nIEEE/ACM International Conference on.\\nIEEE, 2008, pp. 317–326.\\n[14] Y. Y. Lee, S. Harwell, S. Khurshid, and D. Marinov, “Temporal code\\ncompletion and navigation,” in Proceedings of the 2013 International\\nConference on Software Engineering.\\nIEEE Press, 2013, pp. 1181–\\n1184.\\n[15] L. Heinemann and B. Hummel, “Recommending api methods based on\\nidentiﬁer contexts,” in Proceedings of the 3rd International Workshop\\non Search-Driven Development: Users, Infrastructure, Tools, and Eval-\\nuation.\\nACM, 2011, pp. 1–4.\\n[16] M. Asaduzzaman, C. K. Roy, K. A. Schneider, and D. Hou, “Cscc:\\nSimple, efﬁcient, context sensitive code completion,” in 2014 IEEE\\nInternational Conference on Software Maintenance and Evolution (IC-\\nSME).\\nIEEE, 2014, pp. 71–80.\\n[17] D. Hou and D. M. Pletcher, “Towards a better code completion system\\nby api grouping, ﬁltering, and popularity-based ranking,” in Proceedings\\nof the 2nd International Workshop on Recommendation Systems for\\nSoftware Engineering.\\nACM, 2010, pp. 26–30.\\n[18] V. Raychev, M. Vechev, and E. Yahav, “Code completion with statistical\\nlanguage models,” in ACM SIGPLAN Notices, vol. 49, no. 6.\\nACM,\\n2014, pp. 419–428.\\n[19] M. Bruch, M. Monperrus, and Mezini, “Learning from examples to\\nimprove code completion systems,” in Proceedings of the the 7th\\njoint meeting of the European software engineering conference and the\\nACM SIGSOFT symposium on The foundations of software engineering.\\nACM, 2009, pp. 213–222.\\n[20] S. Proksch, J. Lerch, and M. Mezini, “Intelligent code completion with\\nbayesian networks,” ACM Transactions on Software Engineering and\\nMethodology (TOSEM), vol. 25, no. 1, p. 3, 2015.\\n[21] F. McCarey, M. O. Cinn´eide, and N. Kushmerick, “Recommending\\nlibrary methods: An evaluation of bayesian network classiﬁers,” Support-\\ning Knowledge Collaboration in Software Development (KCSD2006),\\np. 17, 2006.\\n[22] M. Sch¨afer, M. Sridharan, J. Dolby, and F. Tip, “Effective smart\\ncompletion for javascript,” Technical Report RC25359, IBM Research,\\nTech. Rep., 2013.\\n[23] C. Franks, Z. Tu, P. Devanbu, and V. Hellendoorn, “Cacheca: A cache\\nlanguage model based code suggestion tool,” in 2015 IEEE/ACM 37th\\nIEEE International Conference on Software Engineering, vol. 2. IEEE,\\n2015, pp. 705–708.\\n[24] M. Bruch, T. Sch¨afer, and M. Mezini, “Fruit: Ide support for framework\\nunderstanding,” in Proceedings of the 2006 OOPSLA workshop on\\neclipse technology eXchange.\\nACM, 2006, pp. 55–59.\\n[25] “32.2. ast abstract syntax trees python 2.7.11 documentation,” https:\\n//docs.python.org/2/library/ast.html, (Accessed on 05/17/2016).\\n[26] “Search — github developer guide,” https://developer.github.com/v3/\\nsearch/, (Accessed on 05/17/2016).\\n[27] “Github,” https://github.com/, (Accessed on 05/18/2016).\\n[28] F. Nielson, H. R. Nielson, and C. Hankin, Principles of program\\nanalysis.\\nSpringer, 2015.\\n[29] “Reaching deﬁnition - wikipedia, the free encyclopedia,” https://en.\\nwikipedia.org/wiki/Reaching deﬁnition, (Accessed on 06/27/2016).\\n[30] E. F. Krause, Taxicab geometry: An adventure in non-Euclidean geom-\\netry.\\nCourier Corporation, 2012.\\n[31] S. Proksch, S. Amann, and M. Mezini, “Towards standardized evalua-\\ntion of developer-assistance tools,” in Proceedings of the 4th Interna-\\ntional Workshop on Recommendation Systems for Software Engineering.\\nACM, 2014, pp. 14–18.\\n[32] M. Bruch, T. Sch¨afer, and M. Mezini, “On evaluating recommender\\nsystems for api usages,” in Proceedings of the 2008 international\\nworkshop on Recommendation systems for software engineering. ACM,\\n2008, pp. 16–20.\\n[33] N. Craswell, “Mean reciprocal rank,” in Encyclopedia of Database\\nSystems.\\nSpringer, 2009, pp. 1703–1703.\\n')]\n"
     ]
    }
   ],
   "source": [
    "# Arxiv Loader\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(query=\"1608.08736\", load_max_docs=2)\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Generative artificial intelligence (generative AI, GenAI, or GAI) is a subset of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.  \n",
      "Improvements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the early 2020s. These include chatbots such as ChatGPT, Copilot, Gemini, and LLaMA; text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora. Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.\n",
      "Generative AI has uses across a wide range of industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. However, concerns have been raised about the potential misuse of generative AI such as cybercrime, the use of fake news or deepfakes to deceive or manipulate people, and the mass replacement of human jobs. Intellectual property law concerns also exist around generative models that are trained on and emulate copyrighted works of art.\n",
      "\n",
      "\n",
      "== History ==\n",
      "\n",
      "\n",
      "=== Early history ===\n",
      "Since its inception, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity. The concept of automated art dates back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music. The tradition of creative automations has flourished throughout history, exemplified by Maillardet's automaton created in the early 1800s. Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is learned on a text corpus, it can then be used as a probabilistic text generator.\n",
      "\n",
      "\n",
      "=== Academic artificial intelligence ===\n",
      "The academic discipline of artificial intelligence was established at a research workshop held at Dartmouth College in 1956 and has experienced several waves of advancement and optimism in the decades since. Artificial Intelligence research began in the 1950s with works like Computing Machinery and Intelligence (1950) and the 1956 Dartmouth Summer Research Project on AI. Since the 1950s, artists and researchers have used artificial intelligence to create artistic works. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings.\n",
      "The terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.\n",
      "\n",
      "\n",
      "=== Generative neural nets (2014-2019) ===\n",
      "\n",
      "Since its inception, the field of machine learning used both discriminative models and generative models, to mode' metadata={'title': 'Generative artificial intelligence', 'summary': 'Generative artificial intelligence (generative AI, GenAI, or GAI) is a subset of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.  \\nImprovements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the early 2020s. These include chatbots such as ChatGPT, Copilot, Gemini, and LLaMA; text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora. Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.\\nGenerative AI has uses across a wide range of industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. However, concerns have been raised about the potential misuse of generative AI such as cybercrime, the use of fake news or deepfakes to deceive or manipulate people, and the mass replacement of human jobs. Intellectual property law concerns also exist around generative models that are trained on and emulate copyrighted works of art.', 'source': 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader  \n",
    "docs = WikipediaLoader(query = \"Generative AI\", load_max_docs=2).load()\n",
    "print(docs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
